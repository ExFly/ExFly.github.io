<search>
    
     <entry>
        <title>Posts</title>
        <url>/post/</url>
        <categories>
          
        </categories>
        <tags>
          
        </tags>
        <content type="html"> </content>
    </entry>
    
     <entry>
        <title>整理了一套 dotfiles 自用</title>
        <url>/post/tools/dotfiles/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>tools</tag><tag>dev</tag>
        </tags>
        <content type="html"> 文章简介：自己的 dotfiles 供自己使用，几乎一键换机
简介 使用方法
git clone https://github.com/exfly/dotfiles.git ~/.dotfiles cd ~/.dotfiles make preinstall-arch make bootstrap make install 备注 对于所有的 *.zsh 都会加载到 .zshrc 中，所有的 *.symlink 都会对应的在$HOME生成一个软连接 .*, 文件夹也可以直接使用这种方式进行加载。
比如 zsh/zshrc.symlink 会生成软连接 $HOME/.zshrc,具体如何工作可以看一下 script/bootstrap
references  my dotfiles </content>
    </entry>
    
     <entry>
        <title>Comtainerd Cgroup Namespace</title>
        <url>/post/docker/comtainerd-cgroup-namespace/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>cgroup</tag><tag>namespace</tag>
        </tags>
        <content type="html"> 文章简介：Docker Cgroup , Namespace , union fs 运行机制
Namespaces 命名空间 (namespaces) 是 Linux 为我们提供的用于分离进程树、网络接口、挂载点以及进程间通信等资源的方法.
Linux 的命名空间机制提供了以下七种不同的命名空间，包括 CLONE_NEWCGROUP、CLONE_NEWIPC、CLONE_NEWNET、CLONE_NEWNS、CLONE_NEWPID、CLONE_NEWUSER 和 CLONE_NEWUTS(分别对应 Cgroup, IPC, Network, Mount, PID, User, UTS)，通过这七个选项我们能在创建新的进程时设置新进程应该在哪些资源上与宿主机器进行隔离。
 the Network namespace encapsulates system resources related to networking such as network interfaces (e.g wlan0, eth0), route tables etc, the Mount namespace encapsulates files and directories in the system, PID contains process IDs and so on. So two instances of a Network namespace A and B (corresponding to two boxes of the same type in our analogy) can contain different resources - maybe A contains wlan0 while B contains eth0 and a different route table copy &amp;ndash; related
 $ ls -l /proc/$$/ns total 0 lrwxrwxrwx 1 vagrant vagrant 0 Sep 19 00:46 cgroup -&amp;gt; &amp;#39;cgroup:[4026531835]&amp;#39; lrwxrwxrwx 1 vagrant vagrant 0 Sep 19 00:46 ipc -&amp;gt; &amp;#39;ipc:[4026531839]&amp;#39; lrwxrwxrwx 1 vagrant vagrant 0 Sep 19 00:46 mnt -&amp;gt; &amp;#39;mnt:[4026531840]&amp;#39; lrwxrwxrwx 1 vagrant vagrant 0 Sep 19 00:46 net -&amp;gt; &amp;#39;net:[4026531992]&amp;#39; lrwxrwxrwx 1 vagrant vagrant 0 Sep 19 00:46 pid -&amp;gt; &amp;#39;pid:[4026531836]&amp;#39; lrwxrwxrwx 1 vagrant vagrant 0 Sep 19 00:46 pid_for_children -&amp;gt; &amp;#39;pid:[4026531836]&amp;#39; lrwxrwxrwx 1 vagrant vagrant 0 Sep 19 00:46 user -&amp;gt; &amp;#39;user:[4026531837]&amp;#39; lrwxrwxrwx 1 vagrant vagrant 0 Sep 19 00:46 uts -&amp;gt; &amp;#39;uts:[4026531838]&amp;#39;$ sudo unshare -u bash $ ls -l /proc/$$/ns total 0 lrwxrwxrwx 1 root root 0 Sep 19 01:00 cgroup -&amp;gt; &amp;#39;cgroup:[4026531835]&amp;#39; lrwxrwxrwx 1 root root 0 Sep 19 01:00 ipc -&amp;gt; &amp;#39;ipc:[4026531839]&amp;#39; lrwxrwxrwx 1 root root 0 Sep 19 01:00 mnt -&amp;gt; &amp;#39;mnt:[4026531840]&amp;#39; lrwxrwxrwx 1 root root 0 Sep 19 01:00 net -&amp;gt; &amp;#39;net:[4026531992]&amp;#39; lrwxrwxrwx 1 root root 0 Sep 19 01:00 pid -&amp;gt; &amp;#39;pid:[4026531836]&amp;#39; lrwxrwxrwx 1 root root 0 Sep 19 01:00 pid_for_children -&amp;gt; &amp;#39;pid:[4026531836]&amp;#39; lrwxrwxrwx 1 root root 0 Sep 19 01:00 user -&amp;gt; &amp;#39;user:[4026531837]&amp;#39; lrwxrwxrwx 1 root root 0 Sep 19 01:00 uts -&amp;gt; &amp;#39;uts:[4026532239]&amp;#39; unshare 会在一个新的 namespace 执行新的程序，flag -u 指定新的 UTS namespace, sudo unshare -u bash即是在新的UTS命名空间执行 bash
linux 以默认的 namespace 启动新的程序，除非明确的指定
user namespace  P$ Q$ 不同的字母代表不同的 user namespace
 P$ whoami vagrant P$ id uid=1000(vagrant) gid=1000(vagrant) groups=1000(vagrant),977(docker) P$ unshare -U bash # Enter a new shell that runs within a nested user namespace, the shell is bash which is your cmd shell C$ id uid=65534(nobody) gid=65534(nobody) groups=65534(nobody) C$ ls -l /proc/$pid/uid_map  the map file /proc/$pid/uid_map returns a mapping from UIDs in the user namespace to which the process pid belongs， user_namespace linux man page
 每一行的格式为 $fromID $toID $length，
C$ echo $$ 8683 # 新的 PID P$ echo $$ 7898 # 原始 PID C$ cat /proc/8683/uid_map C$ cat /proc/7898/uid_map 0 0 4294967295P$ id uid=1000(vagrant) gid=1000(vagrant) groups=1000(vagrant),977(docker) P$ ip link add type veth RTNETLINK answers: Operation not permitted # 在一个不同的username 和 net namespace中尝试 P$ unshare -nU bash C$ ip link add type veth RTNETLINK answers: Operation not permitted C$ echo $$ 9078 P$ echo &amp;#34;0 1000 1&amp;#34; &amp;gt; /proc/9078/uid_map C$ id uid=0(root) gid=65534(nobody) groups=65534(nobody) C$ ip link add type veth # Success! P$ echo deny &amp;gt; /proc/9078/setgroups P$ echo &amp;#34;0 1000 1&amp;#34; &amp;gt; /proc/9078/gid_map C$ id uid=0(root) gid=0(root) groups=0(root),65534(nobody) mount namespace cat /proc/$$/mounts proc /proc proc rw,nosuid,nodev,noexec,relatime 0 0 sys /sys sysfs rw,nosuid,nodev,noexec,relatime 0 0 dev /dev devtmpfs rw,nosuid,relatime,size=496792k,nr_inodes=124198,mode=755 0 0 run /run tmpfs rw,nosuid,nodev,relatime,mode=755 0 0 /dev/sda2 / btrfs rw,relatime,space_cache,subvolid=5,subvol=/ 0 0 securityfs /sys/kernel/security securityfs rw,nosuid,nodev,noexec,relatime 0 0 tmpfs /dev/shm tmpfs rw,nosuid,nodev 0 0 devpts /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000 0 0 tmpfs /sys/fs/cgroup tmpfs ro,nosuid,nodev,noexec,mode=755 0 0 cgroup2 /sys/fs/cgroup/unified cgroup2 rw,nosuid,nodev,noexec,relatime,nsdelegate 0 0 cgroup /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,name=systemd 0 0 pstore /sys/fs/pstore pstore rw,nosuid,nodev,noexec,relatime 0 0 bpf /sys/fs/bpf bpf rw,nosuid,nodev,noexec,relatime,mode=700 0 0 cgroup /sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0 cgroup /sys/fs/cgroup/pids cgroup rw,nosuid,nodev,noexec,relatime,pids 0 0 cgroup /sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0 cgroup /sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0 cgroup /sys/fs/cgroup/rdma cgroup rw,nosuid,nodev,noexec,relatime,rdma 0 0 cgroup /sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0 cgroup /sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0 cgroup /sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0 cgroup /sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0 cgroup /sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0 cgroup /sys/fs/cgroup/hugetlb cgroup rw,nosuid,nodev,noexec,relatime,hugetlb 0 0 hugetlbfs /dev/hugepages hugetlbfs rw,relatime,pagesize=2M 0 0 configfs /sys/kernel/config configfs rw,nosuid,nodev,noexec,relatime 0 0 systemd-1 /proc/sys/fs/binfmt_misc autofs rw,relatime,fd=46,pgrp=1,timeout=0,minproto=5,maxproto=5,direct,pipe_ino=10711 0 0 mqueue /dev/mqueue mqueue rw,nosuid,nodev,noexec,relatime 0 0 debugfs /sys/kernel/debug debugfs rw,nosuid,nodev,noexec,relatime 0 0 tmpfs /tmp tmpfs rw,nosuid,nodev 0 0 tmpfs /run/user/1000 tmpfs rw,nosuid,nodev,relatime,size=100836k,mode=700,uid=1000,gid=1000 0 0 vagrant /vagrant vboxsf rw,relatime 0 0 mount point
$ unshare -m bash $ cat /proc/$$/mounts /dev/sda2 / btrfs rw,relatime,space_cache,subvolid=5,subvol=/ 0 0 dev /dev devtmpfs rw,nosuid,relatime,size=496792k,nr_inodes=124198,mode=755 0 0 tmpfs /dev/shm tmpfs rw,nosuid,nodev 0 0 devpts /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000 0 0 hugetlbfs /dev/hugepages hugetlbfs rw,relatime,pagesize=2M 0 0 mqueue /dev/mqueue mqueue rw,nosuid,nodev,noexec,relatime 0 0 proc /proc proc rw,nosuid,nodev,noexec,relatime 0 0 systemd-1 /proc/sys/fs/binfmt_misc autofs rw,relatime,fd=46,pgrp=1,timeout=0,minproto=5,maxproto=5,direct,pipe_ino=10711 0 0 sys /sys sysfs rw,nosuid,nodev,noexec,relatime 0 0 securityfs /sys/kernel/security securityfs rw,nosuid,nodev,noexec,relatime 0 0 tmpfs /sys/fs/cgroup tmpfs ro,nosuid,nodev,noexec,mode=755 0 0 cgroup2 /sys/fs/cgroup/unified cgroup2 rw,nosuid,nodev,noexec,relatime,nsdelegate 0 0 cgroup /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,name=systemd 0 0 cgroup /sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0 cgroup /sys/fs/cgroup/pids cgroup rw,nosuid,nodev,noexec,relatime,pids 0 0 cgroup /sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0 cgroup /sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0 cgroup /sys/fs/cgroup/rdma cgroup rw,nosuid,nodev,noexec,relatime,rdma 0 0 cgroup /sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0 cgroup /sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0 cgroup /sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0 cgroup /sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0 cgroup /sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0 cgroup /sys/fs/cgroup/hugetlb cgroup rw,nosuid,nodev,noexec,relatime,hugetlb 0 0 pstore /sys/fs/pstore pstore rw,nosuid,nodev,noexec,relatime 0 0 bpf /sys/fs/bpf bpf rw,nosuid,nodev,noexec,relatime,mode=700 0 0 configfs /sys/kernel/config configfs rw,nosuid,nodev,noexec,relatime 0 0 debugfs /sys/kernel/debug debugfs rw,nosuid,nodev,noexec,relatime 0 0 tracefs /sys/kernel/debug/tracing tracefs rw,nosuid,nodev,noexec,relatime 0 0 run /run tmpfs rw,nosuid,nodev,relatime,mode=755 0 0 tmpfs /run/user/1000 tmpfs rw,nosuid,nodev,relatime,size=100836k,mode=700,uid=1000,gid=1000 0 0 tmpfs /tmp tmpfs rw,nosuid,nodev 0 0 vagrant /vagrant vboxsf rw,relatime 0 0 说明 unshare -m bash 并没有按照预期在新的 mount namespace 中运行
所以为了验证mount namespace, 我们需要做如下一些事情:
 创建命令所需的依赖项和系统文件的副本 创建一个新的 mount namespace 将新挂载命名空间中的根文件系统替换为由我们的系统文件副本组成的根文件系统。 在新的安装名称空间内执行程序  进行实验 $ wget http://dl-cdn.alpinelinux.org/alpine/v3.10/releases/x86_64/alpine-minirootfs-3.10.1-x86_64.tar.gz $ mkdir rootfs $ tar -xzf alpine-minirootfs-3.10.1-x86_64.tar.gz -C rootfs $ ls rootfs $ ls rootfs/{mnt,dev,proc,home,sys} Pivot root $ unshare -m bash $ mount --bind rootfs rootfs $ cd rootfs $ mkdir put_old $ pivot_root . put_old $ cd / # We should now have our new root. e.g if we: $ ls proc # proc is empty # And the old root is now in put_old $ ls put_old bin dev home lib lost&#43;found mnt proc run srv tmp var boot etc initrd.img lib64 media opt root sbin sys usr vmlinuz $ /bin/busybox umount -l put_old # 卸载旧的文件系统 $ mount -t proc proc proc/ 到这里，已经创建了一个隔离的 rootfs
进程 $ ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 01:17 ? 00:00:00 /sbin/init root 2 0 0 01:17 ? 00:00:00 [kthreadd] root 3 2 0 01:17 ? 00:00:00 [rcu_gp] root 4 2 0 01:17 ? 00:00:00 [rcu_par_gp] root 5 2 0 01:17 ? 00:00:00 [kworker/0:0-events] root 6 2 0 01:17 ? 00:00:00 [kworker/0:0H-kblockd] root 7 2 0 01:17 ? 00:00:00 [kworker/u4:0-btrfs-endio-write] root 8 2 0 01:17 ? 00:00:00 [mm_percpu_wq] 有两个进程很特殊，pid={1,2}, 这两个进程都是被 Linux 中的上帝进程 idle 创建出来的
 pid=1 的 /sbin/init, 执行内核的一部分初始化工作和系统配置，也会创建一些类似 getty 的注册进程 pid=2 的kthreadd, 管理和调度其他的内核进程  网络 在默认情况下，每一个容器在创建时都会创建一对虚拟网卡，两个虚拟网卡组成了数据的通道，其中一个会放在创建的容器中，会加入到名为 docker0 网桥中。我们可以使用如下的命令来查看当前网桥的接口：
$ brctl show bridge name bridge id STP enabled interfaces br-43ee5ed53f84 8000.024297630322 no docker0 8000.0242bf0282f8 no docker0 会为每一个容器分配一个新的 IP 地址并将 docker0 的 IP 地址设置为默认的网关。网桥 docker0 通过 iptables 中的配置与宿主机器上的网卡相连，所有符合条件的请求都会通过 iptables 转发到 docker0 并由网桥分发给对应的机器。
$ iptables -t nat -L Chain PREROUTING (policy ACCEPT) target prot opt source destination DOCKER all -- anywhere anywhere ADDRTYPE match dst-type LOCAL Chain DOCKER (2 references) target prot opt source destination RETURN all -- anywhere anywhere ip $ ip link list 1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000 link/ether 08:00:27:d4:b6:c8 brd ff:ff:ff:ff:ff:ff 3: eth1: &amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 qdisc fq_codel state DOWN mode DEFAULT group default qlen 1000 link/ether 08:00:27:b7:f5:14 brd ff:ff:ff:ff:ff:ff $ ip netns add coke $ ip netns list coke ip netns list 只显示命名的 netns，初始 netns 是非命名 netns。并且每一个命名 netns 都会在/var/run/netns创建同名文件，这个文件可以让进程切换到此 netns
C$ 代表在一个子命名空间中执行
$ ip netns exec coke bash C$ ip link list 1: lo: &amp;lt;LOOPBACK&amp;gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 C$ ping 127.0.0.1 connect: Network is unreachable C$ ip link set dev lo up C$ ip link list 1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 C$ ping 127.0.0.1 # OK 现在在此 netns 中只可以与同 netns 的进程进行沟通(localhost),我们可以尝试与 init netns 的程序进行沟通
Veth Devices $ ip link add veth0 type veth peer name veth1 # # Create a veth pair (veth0 &amp;lt;=&amp;gt; veth1) $ ip link set veth1 netns coke # # Move the veth1 end to the new namespace C$ ip link list 1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 7: veth1@if5: &amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether ee:16:0c:23:f3:af brd ff:ff:ff:ff:ff:ff link-netnsid 0 $ ip addr add 10.1.1.1/24 dev veth0 现在 veth1 设备已经可以在两个 netns 中看到，为了是他们都可以工作，我们需要给它们两个ip addresses 和让interface up
$ ip addr add 10.1.1.1/24 dev veth0 # In the initial namespace $ ip link set dev veth0 up C$ ip addr add 10.1.1.2/24 dev veth1 # # In the coke namespace C$ ip link set dev veth1 up C$ ip addr show veth1 4: veth1@if5: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 1a:e7:9f:4e:d4:db brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.1.1.2/24 scope global veth1 valid_lft forever preferred_lft forever inet6 fe80::18e7:9fff:fe4e:d4db/64 scope link valid_lft forever preferred_lft forever 现在 veth0 和 veth1 已经 up 和赋予了 ip address 10.1.1.1 10.1.1.2
$ ping -I veth0 10.1.1.2 C$ ping 10.1.1.1 netlink libnetwork chroot https://www.ibm.com/developerworks/cn/linux/l-cn-chroot/index.html
CGroups https://www.ibm.com/developerworks/cn/linux/1506_cgroup/index.html
UnionFS docker export \$(docker create busybox) | tar -C rootfs -xvf - AUFS overlay2
查看系统是否支持相应的文件系统
$ uname -a Linux archlinux 5.1.15-arch1-1-ARCH #1 SMP PREEMPT Tue Jun 25 04:49:39 UTC 2019 x86_64 GNU/Linux $ grep btrfs /proc/filesystems btrfs 有一些系统这种方式找不到对应的系统，但是同样支持此文件系统，比如 archlinux 发行版，Overlay_filesystem, overlay-docker-doc
$ mkdir b0 b1 b2 upper work merged $ mount -t overlay overlay -o lowerdir=./b0:./b1:./b2,upperdir=./upper,workdir=./work ./merged overlay2 将 lowerdir、upperdir、workdir 联合挂载，形成最终的 merged 挂载点，其中 lowerdir 是镜像只读层，upperdir 是容器可读可写层，workdir 是执行涉及修改 lowerdir 执行 copy_up 操作的中转层（例如，upperdir 中不存在，需要从 lowerdir 中进行复制，该过程暂未详细了解，遇到了再分析）
$ tree . ├── b0 ├── b1 ├── b2 ├── merged ├── README.md ├── upper └── work ├── index └── work 获得的文件系统是有层次的，当前的层次关系为：
/upper /b0 /b1 /b2$ echo &amp;#39;192.168.0.1&amp;#39; &amp;gt; b0/ip.txt $ tree . ├── b0 │ └── ip.txt ├── b1 ├── b2 ├── merged │ └── ip.txt ├── README.md ├── upper └── work ├── index └── work $ cat merged/ip.txt 192.168.0.1 $ echo &amp;#39;192.168.0.2&amp;#39; &amp;gt; merged/ip.txt $ tree . ├── b0 │ └── ip.txt ├── b1 ├── b2 ├── merged │ └── ip.txt ├── README.md ├── upper │ └── ip.txt └── work ├── index └── work $ cat upper/ip.txt 192.168.0.2 $ echo &amp;#39;192.168.0.3&amp;#39; &amp;gt; upper/ip.txt $ cat merged/ip.txt 192.168.0.3 $ rm -rf merged/ip.txt $ tree . ├── b0 │ └── ip.txt ├── b1 │ └── ip.txt ├── b2 ├── merged ├── README.md ├── upper │ └── ip.txt └── work ├── index └── work $ ls upper/ip.txt c--------- 1 root root 0, 0 Sep 18 04:35 upper/ip.txt references  container-lab https://draveness.me/docker 调试网络 docker run -it --net container:vibrant_blackburn nicolaka/netshoot https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt overlayfs的一些限制/兼容性问题 linux-namespace-isolate-programing 有一个例子程序 Namespaces in operation cgroup/linux-man-page user-namespace/linum-man-page mount namespaces proc special filesystem management ip netns ip command cheatsheet veth Netlink-doc man netlink
 A deep dive into Linux namespaces
 100个容器周边项目，点亮你的容器集群技能树
 </content>
    </entry>
    
     <entry>
        <title>Log</title>
        <url>/post/architecture/log/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>architecture</tag>
        </tags>
        <content type="html"> 文章简介：log
在多个系统相互协作的分布式系统中，为了能够保证异步系统的协作是否工作，需要一套日志系统保证系统正常的工作。
工具  应用内日志，比如开一个新的 log table，其中记录关键元数据，以及一些返回结果。应用内日志记录的内容未来可以作为数据 migration 的有效工具，保证没有成功的操作可以在未来重试，保证系统的数据的完整 opentracing 跟踪请求链路，方便 debug 分布式系统。（具体如何做，之后补充 TODO: flag） elk 系列 nginx 日志 </content>
    </entry>
    
     <entry>
        <title>Git</title>
        <url>/post/tools/git/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>工具</tag>
        </tags>
        <content type="html"> 文章简介：git 简单使用， 分享一些资料
references  图解 Git Git 交互式学习环境-演示 Git 交互式学习环境-学习中使用 Google Engineering Practices Documentation Conventional Commits 有工具可以生成 changelog </content>
    </entry>
    
     <entry>
        <title>Linker</title>
        <url>/post/cs/linker/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>工具</tag>
        </tags>
        <content type="html"> 文章简介：链接器 学习资源
链接器 学习资源
references  Beginner&amp;rsquo;s Guide to Linkers-cn Beginner&amp;rsquo;s Guide to Linkers
 CppCon 2017: Nir Friedman “What C&#43;&#43; developers should know about globals (and the linker)”
 </content>
    </entry>
    
     <entry>
        <title>消息队列</title>
        <url>/post/architecture/mq/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>mq</tag><tag>architecture</tag><tag>distributed</tag>
        </tags>
        <content type="html"> 文章简介：消息队列中需要解决的问题
网络通信可能会包含，成功、失败以及超时三种情况
消息投递语义 最多一次（At-Most Once）、最少一次（At-Least Once）以及恰好一次（Exactly Once）
最多一次在 TCP/UDP 传输层协议就是保证最多一次消息投递，消息的发送者只会尝试发送该消息一次，并不会关心该消息是否得到了远程节点的响应。 最少一次引入超时重试的机制。同时引入新的问题，消息重复。 恰好一次从理论上来说，在分布式系统中想要解决消息重复的问题是不可能的，很多消息服务提供了正好一次的 QoS 其实是在接收端进行了去重。
投递顺序 由于一些网络的问题，消息在投递时可能会出现顺序不一致性的情况，在网络条件非常不稳定时，我们就可能会遇到接收方处理消息的顺序和生产者投递的不一致；消费者就需要对顺序不一致的消息进行处理，常见的两种方式就是使用序列号或者状态机。
序列号 用阻塞的方式保证序列号的递增或者忽略部分『过期』的消息。
状态机 虽然消息投递的顺序是乱序的，但是资源最终还是通过状态机达到了我们想要的正确状态，不会出现不一致的问题。
协议 AMQP： StormMQ、RabbitMQ， 支持最多一次和最少一次的投递语义，当我们选择最少一次时，需要幂等或者重入机制保证消息重复不会出现问题。
MQTT
reference  分布式事务的实现原理 分布式系统与消息的投递 浅谈数据库并发控制 - 锁和 MVCC 消息队列设计的精髓基本都藏在本文里了 消息队列设计精要 消息队列设计精要 </content>
    </entry>
    
     <entry>
        <title>高可用系统设计问题、微服务设计笔记 脑图</title>
        <url>/post/architecture/microservicedesign/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>Architecture</tag><tag>微服务</tag><tag>Distributed</tag>
        </tags>
        <content type="html"> 高可用系统设计，有哪些问题需要解决
什么是高可用系统 关于高可用的系统
高可用系统需要解决的问题 挑战  成倍的 API 数量   引入网络延迟 CAP 理论，处理跨多个服务的事务复杂 调试分布式系统十分复杂 服务雪崩 大量请求堆积，故障恢复慢 微服务技术选型 API 版本管理混乱 TCC、事务消息队列  需要做的事  可扩展：水平扩展、垂直扩展。 通过冗余部署，避免单点故障。   隔离：避免单一业务占用全部资源。避免业务之间的相互影响 2. 机房隔离避免单点故障。 解耦：降低维护成本，降低耦合风险。减少依赖，减少相互间的影响。 限流：滑动窗口计数法、漏桶算法、令牌桶算法等算法。遇到突发流量时，保证系统稳定。 降级：紧急情况下释放非核心功能的资源。牺牲非核心业务，保证核心业务的高可用。 熔断：异常情况超出阈值进入熔断状态，快速失败。减少不稳定的外部依赖对核心服务的影响。 系统监控：对 CPU 利用率，load，内存，带宽，系统调用量，应用错误量，PV，UV 和业务量进行监控 自动化测试：通过完善的测试，减少发布引起的故障。 灰度发布(&#43;回滚)：灰度发布是速度与安全性作为妥协，能够有效减少发布故障。 异步消息系统  需要深入的技术栈 分布式系统、DevOps、基础架构即代码(IaC)、不同类型的数据库、前端组件化和复合化、单元测试、全自动发布、迭代、小版本发布计划、测试工具、多版本管理
分布式一致性与共识算法 Consensus from wiki, 总结下来一致性就是，在分布式系统中，在给定的一系列操作，即使系统内部出错，最终整个系统对外提供的数据都是可靠的。在协调一致性的过程中，对于一个 Proposal 整个系统达成共识，共识算法起着很重要的作用。
CAP 加州伯克利大学的教授 Eric Brewer 论文Brewer’s Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services 阐述了 CAP 理论 CAP from wiki, 论文中提出的观点：在异步的网络模型中，所有的节点由于没有时钟仅仅能根据接收到的消息作出判断，这时完全不能同时保证一致性(Consistency)、可用性(Availability)和分区容错性(Partition tolerance)，每一个系统只能在这三种特性中选择两种。 由于网络有一定的延迟，并不能做到强一致性，所以大部分时候采用最终一致性的方式，容忍一定时间内数据不一致，在一定的时间内系统内部各节点可以在有限的时间内解决冲突，使数据恢复准确的状态。
拜占庭将军问题 拜占庭将军问题论文
拜占庭将军问题是对分布式系统容错的最高要求，然而这不是日常工作中使用的大多数分布式系统中会面对的问题，我们遇到更多的还是节点故障宕机或者不响应等情况，这就大大简化了系统对容错的要求
FLP FLP 不可能定理是分布式系统领域最重要的定理之一，它给出了一个非常重要的结论：在网络可靠并且存在节点失效的异步模型系统中，不存在一个可以解决一致性问题的确定性算法。
 In this paper, we show the surprising result that no completely asynchronous consensus protocol can tolerate even a single unannounced process death. We do not consider Byzantine failures, and we assume that the message system is reliable it delivers all messages correctly and exactly once.
 相关论文
共识算法 paxos, raft 包括 Paxos、Raft
The Raft Consensus Algorithm raft 工作原理动图
Paxos 算法难以理解、难以实现，难道什么程度呢？在 raft 的论文中有提及。比如 zookeeper 的 zab 就是在 paxos 的基础上进行设计的。每一种 Paxos 的实现，都需要重新设计实现一套算法。而 raft 相对难度降低很多。 基于 raft 的 etcd，consul 等。
POW(Proof-of-Work) 无论是 Paxos 还是 Raft 其实都只能解决非拜占庭将军容错的一致性问题，不能够应对分布式网络中出现的极端情况，但是这在传统的分布式系统都不是什么问题，无论是分布式数据库还是消息队列集群，它们内部的节点并不会故意的发送错误信息，在类似系统中，最常见的问题就是节点失去响应或者失效，所以它们在这种前提下是有效可行的，也是充分的。
工作量证明是一个用于阻止拒绝服务攻击和类似垃圾邮件等服务错误问题的协议
POS(Proof-of-Stake) 权益证明是区块链网络中的使用的另一种共识算法，在基于权益证明的密码货币中，下一个区块的选择是根据不同节点的股份和时间进行随机选择的。
DPOS(Delegated Proof-of-Stake) 《微服务设计》脑图 脑图如下：  脑图地址  references  后端架构师技术图谱 hystrix bilibili/kratos
 Go Microservices blog series
 microservices-in-golang
 shippy-demo
 20 个好用的 Go 语言微服务开发框架
 micro/micro
 micro/go-micro
 CAP theorem
 grpc-example
 分布式一致性与共识算法
 </content>
    </entry>
    
     <entry>
        <title>The Log: What every software engineer should know about real-time data&#39;s unifying abstraction 翻译</title>
        <url>/post/db/log/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>database</tag><tag>distributed</tag>
        </tags>
        <content type="html"> 文章简介：翻译The Log: What every software engineer should know about real-time data&amp;rsquo;s unifying abstraction
正文 六年前我在一个特别有趣的时间加入了 LinkedIn。 我们刚刚开始遇到单体集中式数据库的极限，需要开始向分布式系统过渡。 这是一个有趣的经历：我们构建，部署并运行分布式图形数据库，分布式搜索后端，Hadoop 安装以及第一代和第二代键值存储一直到今天。
我在这一切中学到的最有用的东西之一就是我们正在构建的许多东西都有一个非常简单的概念：日志。 有时称为预写日志或提交日志或事务日志，日志几乎与计算机一样长，并且是许多分布式数据系统和实时应用程序体系结构的核心。
在不了解日志的情况下，您无法完全理解数据库，NoSQL 存储，键值存储，复制，paxos，hadoop，版本控制或几乎任何软件系统; 然而，大多数软件工程师并不熟悉它们。 我想改变这一点。 在这篇文章中，我将向您介绍有关日志的所有信息，包括日志以及如何使用日志进行数据集成，实时处理和系统构建。
Part One: What is Log  A log is perhaps the simplest possible storage abstraction. It is an append-only, totally-ordered sequence of records ordered by time.
 记录附加到日志的末尾，读取从左到右进行。 每个条目都分配有唯一的顺序日志条目号。
记录的顺序定义了“时间”的概念，因为左边的条目被定义为比右边的条目更旧。 日志条目号可以被认为是条目的“时间戳”。 将这种排序描述为时间概念起初看起来有点奇怪，但它具有与任何特定物理时钟分离的便利特性。 当我们进入分布式系统时，此属性将变得至关重要。
出于本讨论的目的，记录的内容和格式并不重要。 此外，我们不能只是继续向日志添加记录，因为我们最终将耗尽空间。 我会稍微回过头来看看。
因此，日志与文件或表格完全不同。 文件是一个字节数组，一个表是一个记录数组，一个日志实际上只是一种表或文件，其中记录按时间排序。
在这一点上，您可能想知道为什么值得谈论这么简单的事情？ 如何以仅附加的记录序列与数据系统相关联？ 答案是日志有一个特定的目的：它们记录发生的事情和时间。 对于分布式数据系统，这在许多方面都是问题的核心。
但在我们走得太远之前，让我澄清一些有点令人困惑的事情。 每个程序员都熟悉另一个日志记录定义 - 应用程序可能使用 syslog 或 log4j 写入本地文件的非结构化错误消息或跟踪信息。 为清楚起见，我将其称为“应用程序日志记录”。 应用程序日志是我描述的日志概念的退化形式。 最大的区别是文本日志主要是供人阅读，而我所描述的“日志”或“数据日志”是为编程访问而构建的。
（实际上，如果你对它进行深入的思考，那么人们读取某个机器上的日志这种理念有些不顺应时代。当涉及到许多服务和服务器的时候，这种方法很快就变成一个难于管理的方式，而且为了认识多个机器的行为，日志的目标很快就变成查询和图形化这些行为的输入了-对多个机器的某些行为而言，文件里的英文形式的文本同这儿所描述的这种结构化的日志相比几乎就不适合了。）
Logs in databases 我不知道日志概念的起源 - 可能是二元搜索这样的事情之一，对于发明者来说太简单了，不能发现它是一项发明。 它早在 IBM 的 System R 就已存在。数据库中的使用与在出现崩溃时保持同步的各种数据结构和索引有关。 为了使这种原子性和持久性，数据库使用日志来写出有关他们将要修改的记录的信息，然后将更改应用于它维护的所有各种数据结构。 日志是发生的事件的记录，每个表或索引是将该历史记录投影到一些有用的数据结构或索引中。 由于日志会立即保留，因此在崩溃时将其用作恢复所有其他持久性结构的权威来源。
随着时间的推移，日志的使用从 ACID 的实现细节发展到在数据库之间复制数据的方法。 事实证明，数据库上发生的更改顺序正是保持远程副本数据库同步所需的。 Oracle，MySQL 和 PostgreSQL 包括日志传送协议，用于将部分日志传输到充当从属服务器的副本数据库。 Oracle 已将日志产品化为非 oracle 数据订阅者的通用数据订阅机制，其XStream和GoldenGate以及 MySQL 和 PostgreSQL 中的类似工具是许多数据架构的关键组件。
由于这个起源，机器可读日志的概念主要局限于数据库内部。 使用日志作为数据订阅的机制似乎几乎是偶然出现的。 但这种抽象是支持各种消息传递，数据流和实时数据处理的理想选择。
Logs in distributed systems 日志解决的两个问题 - 排序更改(ordering changes)和分发数据(distributing data) - 在分布式数据系统中更为重要。 同意订购更新（或同意不同意和应对副作用）是这些系统的核心设计问题。
分布式系统的以日志为中心的方法源于一个简单的讨论，我称之为状态机复制原则：
 If two identical, deterministic processes begin in the same state and get the same inputs in the same order, they will produce the same output and end in the same state.
如果两个相同的确定性过程以相同的状态开始并以相同的顺序获得相同的输入，则它们将产生相同的输出并以相同的状态结束。
 这可能看起来有点难懂的(obtuse)，所以让我们深入了解它的含义。
Deterministic(确定性)意味着处理不依赖于时间，并且不允许任何其他“外部的”输入影响其结果。 例如，一个程序的输出受到线程执行的特定顺序的影响，或者通过调用 gettimeofday 或其他一些不可重复的东西，通常被认为是非确定性的。
进程的 state 状态是处理结束时机器上的任何数据，无论是在内存中还是在磁盘上。
以相同的顺序获得相同输入的地方应当引起注意-这就是引入日志的地方。这儿有一个重要的常识：如果给两段确定性代码相同的日志输入，那么它们就会生成相同的输出。
分布式计算这方面的应用就格外明显。你可以把用多台机器一起执行同一件事情的问题缩减为实现分布式一致性日志为这些进程输入的问题。这日志的目的是把所有非确定性的东西排除在输入流之外，来确保每个复制进程能够同步地处理输入。
当你理解了这个以后，状态机复制原理就不再复杂或者说不再深奥了：这或多或少的意味着&amp;quot;deterministic processing is deterministic&amp;quot;(确定性的处理过程就是确定性的)。不管怎样，我都认为它是分布式系统设计里较常用的工具之一。
这种方式的一个美妙之处就在于索引日志的时间戳就像时钟状态的一个副本——你可以用一个单独的数字描述每一个副本，这就是经过处理的日志的时间戳。时间戳与日志一一对应着整个副本的状态。
由于写进日志的内容的不同，也就有许多在系统中应用这个原则的不同方式。举个例子，我们记录一个服务的请求，或者服务从请求到响应的状态变化，或者它执行命令的转换。理论上来说，我们甚至可以为每一个副本记录一系列要执行的机器指令或者调用的方法名和参数。只要两个进程用相同的方式处理这些输入，这些进程就会保持副本的一致性。
一千个人眼中有一千种日志的用法。数据库工作者通常区分物理日志和逻辑日志。物理日志就是记录每一行被改变的内容。逻辑日志记录的不是改变的行而是那些引起行的内容被改变的 SQL 语句（insert，update 和 delete 语句）。
分布式系统通常可以宽泛分为两种方法来处理数据和完成响应。“状态机器模型”通常引用一个主动-主动的模型——也就是我们为之记录请求和响应的对象。对此进行一个细微的更改，称之为“主备模型”，就是选出一个副本做为 leader，并允许它按照请求到达的时间来进行处理并从处理过程中输出记录其状态改变的日志。其他的副本按照 leader 状态改变的顺序而应用那些改变，这样他们之间达到同步，并能够在 leader 失败的时候接替 leader 的工作。
为了理解两种方式的不同，我们来看一个不太严谨的例子。假定有一个算法服务的副本，保持一个独立的数字作为它的状态（初始值为 0），并对这个值进行加法和乘法运算。主动-主动方式应该会输出所进行的变换，比如“&#43;1”，“*2”等。每一个副本都会应用这些变换，从而得到同样的解集。主动-被动方式将会有一个独立的主体执行这些变换并输出结果日志，比如“1”，“3”，“6”等。这个例子也清楚的展示了为什么说顺序是保证各副本间一致性的关键：一次加法和乘法的顺序的改变将会导致不同的结果。
分布式日志可以理解为一致性问题模型的数据结构。因为日志代表了后续追加值的一系列决策。你需要重新审视 Paxos 算法簇，尽管日志模块是他们最常见的应用。 在 Paxos 算法中，它通常通过使用称之为多 paxos 的协议，这种协议将日志建模为一系列的问题，在日志中每个问题都有对应的部分。在ZAB， Raft等其它的协议中，日志的作用尤为突出，它直接对维护分布式的、一致性的日志的问题建模。
我怀疑的是，我们就历史发展的观点是有偏差的，可能是由于过去的几十年中，分布式计算的理论远超过了其实际应用。在现实中，共识的问题是有点太简单了。计算机系统很少需要决定单个值，他们几乎总是处理成序列的请求。这样的记录，而不是一个简单的单值寄存器，自然是更加抽象。
此外，专注于算法掩盖了 抽象系统需要的底层的日志。我怀疑，我们最终会把日志中更注重作为一个商品化的基石，不论其是否以同样的方式 实施的，我们经常谈论一个哈希表而不是纠结我们 得到是不是具体某个细节的哈希表，例如线性或者带有什么什么其它变体哈希表。日志将成为一种大众化的接口，为大多数算法和其实现提升提供最好的保证和最佳的性能。
Changelog 101: Tables and Events are Dual 让我们继续聊数据库。数据库中存在着大量变更日志和表之间的二相性。这些日志有点类似借贷清单和银行的流程，数据库表就是当前的盈余表。如果你有大量的变更日志，你就可以使用这些变更用以创建捕获当前状态的表。这张表将记录每个关键点（日志中一个特别的时间点）的状态信息。这就是为什么日志是非常基本的数据结构的意义所在：日志可用来创建基本表，也可以用来创建各类衍生表。同时意味着可以存储非关系型的对象。
这个流程也是可逆的：如果你正在对一张表进行更新，你可以记录这些变更，并把所有更新的日志发布到表的状态信息中。这些变更日志就是你所需要的支持准实时的克隆。基于此，你就可以清楚的理解表与事件的二相性： 表支持了静态数据而日志捕获变更。日志的魅力就在于它是变更的完整记录，它不仅仅捕获了表的最终版本的内容，它还记录了曾经存在过的其它版本的信息。日志实质上是表历史状态的一系列备份。
这可能会引起你对源代码的版本管理。源代码管理和数据库之间有密切关系。版本管理解决了一个大家非常熟悉的问题，那就是什么是分布式数据系统需要解决的&amp;mdash; 时时刻刻在变化着的分布式管理。版本管理系统通常以补丁的发布为基础，这实际上可能是一个日志。您可以直接对当前 类似于表中的代码做出“快照”互动。你会注意到， 与其他分布式状态化系统类似，版本控制系统 当你更新时会复制日志，你希望的只是更新补丁并将它们应用到你的当前快照中。
最近，有些人从Datomic &amp;ndash; 一家销售日志数据库的公司得到了一些想法。这个演讲回顾了他们是如何讲这个想法应用到他们的系统中的。这些想法使他们对如何 在他们的系统应用这些想法有了开阔的认识。 当然这些想法不是只针对这个系统，他们会成为 十多年分布式系统和数据库文献的一部分。
这可能似乎有点过于理想化。但是不要悲观！我们会很快把它实现。
What&amp;rsquo;s next 在这篇文章的其余部分，我将试图说明日志除了可用在分布式计算或者抽象分布式计算模型内部之外，还可用在哪些方面。其中包括：
 数据集成(Data Integration)-让机构的全部存储和处理系统里的所有数据很容易地得到访问。 实时数据处理(Real-time data processing)-计算生成的数据流。 分布式系统设计(Distributed system design)-实际应用的系统是如何通过使用集中式日志来简化设计的。  所有这些用法都是通过把日志用做单独服务来实现的。
在上面任何一种用法里，日志的用途开始都是使用了日志所能提供的某个简单功能：生成永久的、可重现的历史记录。令人意外的是，问题的核心是可以让多少台机器以特定的方式，按照自身的速度重现历史记录的能力。
Part Two: Data Integration 请让我首先解释 一下“数据集成”是什么意思，还有为什么我觉得它很重要，之后我们再来看看它和日志有什么关系。
 Data integration is making all the data an organization has available in all its services and systems.
数据集成就是将数据组织起来，使得在与其有关的服务和系统中可以访问它们
 而更常见的术语 ETL 通常只是覆盖了数据集成的一个有限子集(译注：ETL，Extraction-Transformation-Loading 的缩写，即数据提取、转换和加载)——相对于关系型数据仓库。但我描述的东西很大程度上可以理解为，将 ETL 推广至实时系统和处理流程。
对数据的高效使用遵循一种 马斯洛的需要层次理论 。金字塔的基础部分包括捕获所有相关数据，能够将它们全部放到适当的处理环境（那个环境应该是一个奇妙的实时查询系统，或者仅仅是文本文件和 python 脚本）。这些数据需要以统一的方式建模，这样就可以方便读取和数据处理。如果这种以统一的方式捕获数据的基本需求得到满足，那么就可以在基础设施上以若干种方法处理这些数据——映射化简（MapReduce），实时查询系统，等等。
很明显，有一点值得注意：如果没有可靠的、完整的数据流，Hadoop 集群除了象昂贵的且难于安装的空间取暖器哪样外不会做更多事情了。一旦数据和处理可用，人们就会关心良好数据模型和一致地易于理解的语法哪些更细致的问题。最后，人们才会关注更加高级的处理-更好的可视化、报表以及处理和预测算法。 以我的经验，大多数机构在数据金字塔的底部存在巨大的漏洞-它们缺乏可靠的、完整的数据流-而是打算直接跳到高级数据模型技术上。这样做完全是反着来做的。
因此，问题是我们如何构建通过机构内所有数据系统的可靠的数据流。
Data Integration: Two complications 两种趋势使数据集成变得更困难。
第一个趋势是增长的事件数据(event data)。事件数据记录的是发生的事情，而不是存在的东西。在 web 系统中，这就意味着用户活动日志，还有为了可靠的操作以及监控数据中心的机器的目的，所需要记录的机器级别的事件和统计数字。人们倾向称它们为“日志数据”，因为它们经常被写到应用的日志中，但是这混淆了形式与功能。这种数据位于现代 web 的中心：归根结底，Google 的资产是由这样一些建立在点击和映像基础之上的相关管道所生成的——那也就是事件。
这些东西并不是仅限于网络公司，只是网络公司已经完全数字化，所以它们更容易用设备记录。财务数据一直是面向事件的。RFID(无线射频识别)将这种跟踪能力赋予物理对象。我认为这种趋势仍将继续，伴随着这个过程的是传统商务活动的数字化。
这种类型的事件数据记录下发生的事情，而且往往比传统数据库应用要大好几个数量级。这对于处理提出了重大挑战。
TThe explosion of specialized data systems 第二个趋势来自于专门的数据系统的爆发，通常这些数据系统在最近的五年中开始变得流行，并且可以免费获得。专门的数据系统是为OLAP, 搜索, 简单 在线 存储, 批处理, 图像分析, 等 等 而存在的。
更多的不同类型数据的组合，以及将这些数据存放到更多的系统中的愿望，导致了一个巨大的数据集成问题。
Log-structured data flow 为了处理系统之间的数据流，日志是最自然的数据结构。其中的秘诀很简单：
将所有组织的数据提取出来，并将它们放到一个中心日志，以便实时查阅。
每个逻辑数据源都可以建模为它自己的日志。一个数据源可以是一个应用程序的事件日志（如点击量或者页面浏览量），或者是一个接受修改的数据库表。每个订阅消息的系统都尽可能快的从日志读取信息，将每条新的记录保存到自己的存储，并且提升其在日志中的地位。订阅方可以是任意一种数据系统 —— 一个缓存，Hadoop，另一个网站中的另一个数据库，一个搜索系统，等等。
例如，日志针对每个更改给出了逻辑时钟的概念，这样所有的订阅方都可以被测量。推导不同的订阅系统的状态也因此变得相对简单的多，因为每个系统都有一个读取动作的“时间点”。
为了让这个显得更具体，我们考虑一个简单的案例，有一个数据库和一组缓存服务器集群。日志提供了一种同步更新所有这些系统，并推导出每一个系统的接触时间点的方法。我们假设写了一条日志 X，然后需要从缓存做一次读取。如果我们想保证看到的不是陈旧的数据，我们只需保证没有从任何尚未复制 X 的缓存中读取即可。
日志也起到缓存的作用，使数据生产与数据消费相同步。由于许多原因这个功能很重要，特别是在多个订阅方消费数据的速度各不相同的时候。这意味着一个订阅数据系统可以宕机，或者下线维护，之后重新上线以后再赶上来：订阅方按照自己控制的节拍来消费数据。批处理系统，如 Hadoop 或者是一个数据仓库，或许只是每小时或者每天消费一次数据，而实时查询系统可能需要及时到秒。由于无论是原始数据源还是日志，都没有各种目标数据系统的相关知识，因此消费方系统可以被添加和删除，而无需传输管道的变化。
 Eache working data pipeline is designed like a log; each broken data pipeline is broken is its own way &amp;ndash; Anna Karenina principle
 特别重要的是：目标系统只知道日志而不知道原始系统的任何细节。 消费者系统不需要关心数据是来自 RDBMS，新的键值存储，还是没有任何类型的实时查询系统。 这似乎是一个小问题，但实际上是需要仔细鉴别的(critical)。
这里我使用术语“日志”取代了“消息系统”或者“发布-订阅”，因为它在语义上更明确，并且对支持数据复制的实际实现这样的需求，有着更接近的描述。我发现“发布订阅”并不比间接寻址的消息具有更多的含义——如果你比较任何两个发布-订阅的消息传递系统的话，你会发现他们承诺的是完全不同的东西，而且大多数模型在这一领域都不是有用的。你可以认为日志是一种消息系统，它具有持久性保证和强大的订阅语义。在分布式系统中，这个通信模型有时有个(有些可怕的)名字叫做atomic broadcast。
值得强调的是，日志仍然只是基础设施。这并不是管理数据流这个故事的结束：故事的其余部分围绕着元数据，模式，兼容性，以及处理数据结构的所有细节及其演化。除非有一种可靠的，一般的方法来处理数据流运作，语义在其中总是次要的细节。
At LinkedIn 在 LinkedIn 从集中式关系数据库向分布式系统集合转化的过程中，我看到这个数据集成问题迅速演变。
主要的数据系统包括：
 Search Social Graph Voldemort (key-value store) Espresso (document store) Recommendation engine OLAP query engine Hadoop Terradata Ingraphs (monitoring graphs and metrics services)  这种使用日志作为数据流的思想，甚至在我到这里之前就已经与 LinkedIn 相伴了。我们开发的一个最早的基础设施之一，是一种称为 databus 的服务，它在我们早期的 Oracle 表上提供了一种日志缓存抽象，可伸缩订阅数据库修改,这样我们就可以很好支持我们的社交网络和搜索索引。
我会给出一些历史并交代一下上下文。我首次参与到这些大约是在 2008 年左右，在我们转移键值存储之后。我的下一个项目是让一个工作中的 Hadoop 配置演进，并给其增加一些我们的推荐流程。由于缺乏这方面的经验，我们自然而然的安排了数周计划在数据的导入导出方面，剩下的时间则用来实现奇妙的预测算法。这样我们就开始了长途跋涉。
我们本来计划是仅仅将数据从现存的 Oracle 数据仓库中剖离。但是我们首先发现将数据从 Oracle 中迅速取出是一种黑暗艺术。更糟的是，数据仓库的处理过程与我们为 Hadoop 而计划的批处理生产过程不适合——其大部分处理都是不可逆转的，并且与即将生成的报告具体相关。最终我们采取的办法是，避免使用数据仓库，直接访问源数据库和日志文件。最后，我们为了加载数据到键值存储 并生成结果，实现了另外一种管道。
这种普通的数据复制最终成为原始开发项目的主要内容之一。糟糕的是，在任何时间任意管道都有一个问题，Hadoop 系统很大程度上是无用的——在错误的数据基础上运行奇特的(fancy)算法，只会产生更多的错误数据。
虽然我们已经以一种通用的方式创建事物，但是每个数据源都需要自定义配置安装。这也被证明是巨量错误与失败的根源。我们在 Hadoop 上实现的网站功能已经开始流行起来，同时我们发现我们有一长串感兴趣的工程师。每个用户都有他们想要集成的一系列系统，他们想要的一系列新数据源。
 ETL in Ancient Greece. Not much has changed.
 有些东西在我面前开始渐渐清晰起来。
首先，我们已建成的通道虽然有一些杂乱，但实质上它们是很有价值的。在采用诸如 Hadoop 的新的处理系统生成可用数据的过程，它解锁了大量的可能性。 基于这些数据过去很难实现的计算，如今变为可能。 许多新的产品和分析技术都来源于把分片的数据放在一起，这些数据被锁定在特定的系统中。
第二， 众所周知，可靠的数据加载需要数据通道的深度支持。如果我们可以捕获所有我们需要的结构，我就可以使得 Hadoop 数据全自动的加载，这样就不需要额外的操作来增加新的数据源或者处理模式变更&amp;ndash;数据就会自动的出现在 HDFS，Hive 表就会自动的生成对应于新数据源的恰当的列。
第三，我们的数据覆盖率仍然非常低。如果你查看存储于 Hadoop 中的可用的 Linked 数据的全部百分比，它仍然是不完整的。花费大量的努力去使得各个新的数据源运转起来，使得数据覆盖度完整不是一件容易的事情。
我们正在推行的，为每个数据源和目标增建客户化数据加载，这种方式很显然是不可行的。我们有大量的数据系统和数据仓库。把这些系统和仓库联系起来，就会导致任意一对系统会产生如下所示的客户化通道。
需要注意的是：数据是双向流动的：例如许多系统诸如数据库和 Hadoop 既是数据转化的来源又是数据转化的目的地。这就意味着我们我们不必为每个系统建立两个通道：一个用于数据输入，一个用于数据输出。
这显然需要一大群人，而且也不具有可操作性。随着我们接近完全连接，最终我们将有差不多 O(N2)条管道。
Instead，我们需要像这样通用的东西：
我们需要尽可能的将每个消费者与数据源隔离。理想情形下，它们应该只与一个单独的数据仓库集成，并由此让他们能访问到所有东西。
这个思想是增加一个新的数据系统——或者它是一个数据源或者它是一个数据目的地——让集成工作只需连接到一个单独的管道，而无需连接到每个数据消费方。
在相当长的时间内，Kafka 是独一无二的底层产品，它既不是数据库，也不是日志文件收集系统，更不是传统的消息系统。但是最近 Amazon 提供了非常类似 Kafka 的服务，称之为 Kinesis.相似度包括了分片处理的方式，数据的保持，甚至包括在 Kafka API 中，有点特别的高端和低端消费者分类。我很开心看到这些，这表明了你已经创建了很好的底层协议，AWS 已经把它作为服务提供。他们对此的期待与我所描述的吻合：通道联通了所有的分布式系统，诸如 DynamoDB, RedShift, S3 等，它同时作为使用 EC2 进行分布式流处理的基础。
Relationship to ETL and the Data Warehouse 我们再来聊聊数据仓库。数据仓库是清洗和归一数据结构用于支撑数据分析的仓库。这是一个伟大的理念。对不熟悉数据仓库概念的人来说，数据仓库方法论包括了：周期性的从数据源抽取数据，把它们转化为可理解的形式，然后把它导入中心数据仓库。对于数据集中分析和处理，拥有高度集中的位置存放全部数据的原始副本是非常宝贵的资产。在高层级上，也许你抽取和加载数据的顺序略微调整，这个方法论不会有太多变化,无论你使用传统的数据仓库 Oracle 还是 Teradata 或者 Hadoop。
数据仓库是极其重要的资产，它包含了原始的和规整的数据，但是实现此目标的机制有点过时了。
对以数据为中心的组织关键问题是把原始的归一数据联结到数据仓库。数据仓库是批处理的基础查询：它们适用于各类报表和临时性分析，特别是当查询包含了简单的计数、聚合和过滤。但是如果一个批处理系统仅仅包含了原始的完整的数据的数据仓库，这就意味着这些数据对于实时数据处理、搜索索引和系统监控等实时的查询是不可用的。
依我之见，ETL 包括两件事：首先，它是抽取和数据清洗过程&amp;ndash;特别是释放被锁在组织的各类系统中的数据，移除系统专有的无用物。第二，依照数据仓库的查询重构数据。例如使其符合关系数据库类型系统，强制使用星号、雪花型模式，或者分解为高性能的柱状格式等。合并这两者是有困难的。这些规整的数据集应当可以在实时或低时延处理中可用，也可以在其它实施存储系统索引。
在我看来，正是因为这个原因有了额外好处：使得数据仓库 ETL 更具了组织级的规模。数据仓库的精典问题是数据仓库负责收集和清洗组织中各个组所生成的全部数据。各组织的动机是不同的，数据的生产者并不知晓在数据仓库中数据的使用情况，最终产生的数据很难抽取，或者需要花费规模化的转化才可以转化为可用的形式。当然， 中心团队不可能恰到好处的掌握规模，使得这规模刚好与组织中其它团队相匹配，因此数据的覆盖率常常差别很大，数据流是脆弱的同时变更是缓慢的。
较好的方法是有一个中心通道，日志和用于增加数据的定义良好的 API。与通道集成的且提供良好的结构化的数据文件的职责依赖于数据的生产者所生成的数据文件。这意味着在设计和实施其它系统时应当考虑数据的输出以及输出的数据如何转化为结构良好的形式并传递给中心通道。增加新的存储系统倒是不必因为数据仓库团队有一个中心结点需要集成而关注数据仓库团队。数据仓库团队仅需处理简单的问题，例如从中心日志中加载结构化的数据，向其它周边系统实施个性化的数据转化等。
如图所示：当考虑在传统的数据仓库之外增加额外的数据系统时，组织结构的可扩展性显得尤为重要。例如，可以考虑为组织的完整的数据集提供搜索功能。或者提供二级的数据流监控实时数据趋势和告警。无论是这两者中的哪一个，传统的数据仓库架构甚至于 Hadoop 聚簇都不再适用。更糟的是，ETL 的流程通道的目的就是支持数据加载，然而 ETL 似乎无法输出到其它的各个系统，也无法通过引导程序，使得这些外围的系统的各个架构成为适用于数据仓库的重要资产。这就不难解释为什么组织很难轻松的使用它的全部数据。反之，如果组织已建立起了一套标准的、结构良好的数据，那么任何新的系统要使用这些数据仅仅需要与通道进行简单的集成就可以实现。
这种架构引出了数据清理和转化在哪个阶段进行的不同观点：
 由数据的生产者在把数据增加到公司全局日志之前。 在日志的实时转化阶段进行，这将会产生一个新的转化日志。 在向目标系统加载数据时，做为加载过程的一部分进行。  理想的模形是：由数据的生产者在把数据发布到日志之前对数据进行清理。这样可以确保数据的权威性，不需要维护其它的遗留物例如为数据产生的特殊处理代码或者维护这些数据的其它的存储系统。这些细节应当由产生数据的团队来处理，因为他们最了解他们自己的数据。这个阶段所使用的任何逻辑都应该是无损的和可逆的。
任何可以实时完成的增值转化类型都应当基于原始日志进行后期处理。这一过程包括了事件数据的会话流程，或者增加大众感兴趣的衍生字段。原始的日志仍然是可用的，但是这种实时处理产生的衍生日志包含了参数数据。
最终，只有针对目标系统的聚合需要做了加载流程的一部分。它包括了把数据转化成特定的星型或者雪花状模式，从而用于数据仓库的分析和报表。因为在这个阶段，大部分自然的映射到传统的 ETL 流程中，而现在它是在一个更加干净和规整的数据流集在进行的，它将会更加的简单。
Log Files and Events 我们再来聊聊这种架构的优势：它支持解耦和事件驱动的系统。
在网络行业取得活动数据的典型方法是把它记为文本形式的日志，这些文本文件是可分解进入数据仓库或者 Hadoop，用于聚合和查询处理的。由此产生的问题与所有批处理的 ETL 的问题是相同的：它耦合了数据流进入数据仓库系统的能力和流程的调度。
在 LinkedIn 中，我们已经以中心日志的方式构建了事件数据处理。我们正在使用 Kafka 做为中心的、多订阅者事件日志。我们已经定义了数百种事件类型，每种类型都会捕获用于特定类型动作的独特的属性。这将会覆盖包括页面视图、表达式、搜索以及服务调用、应用异常等方方面面。
为了进一步理解这一优势：设想一个简单的事务&amp;ndash;在日志页面显示已发布的日志。这个日志页面应当只包括显示日志所需要的逻辑。然而，在相当多的动态站点中，日志页面常常变的添加了很多与显示日志无关的逻辑。例如，我们将对如下的系统进行集成：
 需要把数据传送到 Hadoop 和数据仓库中用于离线数据处理。   需要对视图进行统计，确保视图订阅者不会破坏一些内容片段。 需要聚合这些视图，视图将用于作业发布者的分析页面显示。 需要记录视图以确保我们为作业推荐的使用者提供了恰当的印象覆盖，我们不想一次次的重复同样的事情。 推荐系统需要记录日志用于正确的跟踪作业的普及度。 等等。  不久，简单的作业显示变得相当的复杂。我们增加了作业显示的其它终端&amp;ndash;移动终端应用等&amp;ndash;这些逻辑必须继续存在，复杂度不断的增加。更糟的是我们需要与之做接口交互的系统现在是错综复杂的&amp;ndash;在为显示日作业而工作的工程师们需要知晓多个其它系统和它们的特征，才可以确保它们被正确的集成了。这仅仅是问题的简单版本，真实的的应用系统只会更加的复杂。
“事件驱动”的模式提供了一种简化这类问题的机制。作业显示页面现在只显示作业并记录与正在显示的作业，作业订阅者相关的其它属性，和其它与作业显示相关的其它有价值的属性。每个与此相关的其它系统诸如推荐系统、安全系统、作业推送分析系统和数据仓库，所有这些只是订阅种子文件，并进行它们的操作。显示代码并不需要关注其它的系统，也不需要因为增加了数据的消费者而相应的进行变更。
Building a Scalable Log 当然，把发布者与订阅者分离不再是什么新鲜事了。但是如果你想要确保提交日志的行为就像多个订阅者实时的分类日志那样记录网站发生的每件事时，可扩展性就会成为你所面临的首要挑战。如果我们不能创建快速、高性价比和可扩展性灵活的日志以满足实际的可扩展需求，把日志做为统一的集成机制不再是美好的想像，
人们普遍认为分布式日志是缓慢的、重量经的概念（并且通常会把它仅仅与“原数据”类型的使用联系起来，对于这类使用 Zookeeper 可以适用）。但是深入实现并重点关注分类记录大规模的数据流，这种需求是不切实际的。在 LinkedIn, 我们现在每天通过 Kafka 运行着超过 600 亿个不同的消息写入点(如果统计镜相与数据中心之间的写入，那么这个数字会是数千亿。)
我们在 Kafk 中使用了一些小技巧来支持这种可扩展性：
 日志分片   通过批处理读出和写入优化吞吐力 规避无用的数据复制  为了确保水平可扩展性，我们把日志进行切片：
每个切片都是一篇有序的日志，但是各片之间没有全局的次序（这个有别于你可能包含在消息中的挂钟时间）。把消息分配到特定的日志片段这是由写入者控制的，大部分使用者会通过用户 ID 等键值来进行分片。分片可以把日志追加到不存在协作的片段之间，也可以使系统的吞吐量与 Kafka 聚簇大小成线性比例关系。
每个分片都是通过可配置数量的复制品复制的，每个复制品都有分片的一份完全一致的拷贝。无论何时，它们中的任一个都可以做为主分片，如果主分片出错了，任何一个复制品都可以接管并做为主分片。
缺少跨分片的全局顺序是这个机制的局限性，但是我们不认为它有多重要。事实上，与日志的交互主要来源于成百上千个不同的流程，以致于对于它们的行为排一个总体的顺序是没什么意义的。相反，我们可以确保的是我们提供的每个分片都是按顺序保留的。Kafka 保证了追加到由单一发送者送出的特定分片会按照发送的顺序依次处理。
日志，就像文件系统一样，是容易优化成线性可读可写的样式的。日志可以把小的读入和写出组合成大的、高吞吐量的操作。Kafka 一直至立于实现这一优化目标。批处理可以发生在由客户端向服务器端发送数据、写入磁盘;在服务器各端之间复制；数据传递给消费者和确认提交数据等诸多环节。
最终，Kafka 使用简单的二进制形式维护内存日志，磁盘日志和网络数据传送。这使得我们可以使用包括“0 数据复制传送”在内的大量的优化机制。
这些优化的积累效应是你常常进行的写出和读入数据的操作可以在磁盘和网络上得到支持，甚至于维护内存以外的大量数据集。 这些详细记述并不意味着这是关于 Kafka 的主要内容，那么我就不需要了解细节了。你可在这个链接阅读到更多的关于 LinkedIn 的方法，和这个链接是关于 Kafka 的设计总述。
Part Three: Logs &amp;amp; Real-time Stream Processing 到此为止，我只是描述从端到端数据复制的理想机制。但是在存储系统中搬运字节不是所要讲述内容的全部。最终我们发现日志是流的另一种说法，日志是流处理的核心。
但是，等等，什么是流处理呢？
如果你是 90 年代晚期或者 21 世纪初数据库文化或者数据基础架构产品的爱好者，那么你就可能会把流处理与建创 SQL 引擎或者创建“箱子和箭头”接口用于事件驱动的处理等联系起来。
如果你关注开源数据库系统的大量出现，你就可能把流处理和一些开源数据库系统关联起来，这些系统包括了：Storm,Akka,S4和Samza.但是大部分人会把这些系统作为异步消息处理系统，这些系统与支持群集的远程过程调用层的应用没什么差别（而事实上在开源数据库系统领域某些方面确实如此）。
这些视图都有一些局限性。流处理与 SQL 是无关的。它也局限于实时流处理。不存在内在的原因限制你不能处理昨天的或者一个月之前的流数据，且使用多种不同的语言表达计算。
我把流处理视为更广泛的概念：持续数据流处理的基础架构。我认为计算模型可以像 MapReduce 或者分布式处理架构一样普遍，但是有能力处理低时延的结果。
处理模型的实时驱动是数据收集方法。成批收集的数据是分批处理的。数据是不断收集的，它也是按顺序不断处理的。
美国的统计调查就是成批收集数据的良好典范。统计调查周期性的开展，通过挨门挨户的走访，使用蛮力发现和统计美国的公民信息。1790 年统计调查刚刚开始时这种方式是奏效的。那时的数据收集是批处理的，它包括了骑着马悠闲的行进，把信息写在纸上，然后把成批的记录传送到人们统计数据的中心站点。现在，在描述这个统计过程时，人们立即会想到为什么我们不保留出生和死亡的记录，这样就可以产生人口统计信息这些信息或是持续的或者是其它维度的。
这是一个极端的例子，但是大量的数据传送处理仍然依赖于周期性的转储，批量转化和集成。处理大容量转储的唯一方法就是批量的处理。但是随着这些批处理被持续的供给所取代，人们自然而然的开始不间断的处理以平滑的处理所需资源并且消除延迟。
例如 LinkedIn 几乎没有批量数据收集。大部分的数据或者是活动数据或者是数据库变更，这两者都是不间断发生的。事实上，你可以想到的任何商业，正如：Jack Bauer 告诉我们的，低层的机制都是实时发生的不间断的流程事件。数据是成批收集的，它总是会依赖于一些人为的步骤，或者缺少数字化或者是一些自动化的非数字化流程处理的遗留信息。当传送和处理这些数据的机制是邮件或者人工的处理时，这一过程是非常缓慢的。首轮自动化总是保持着最初的处理形式，它常常会持续相当长的时间。
每天运行的批量处理作业常常是模拟了一种一天的窗口大小的不间断计算。当然，低层的数据也经常变化。在 LinkedIn,这些是司空见贯的，并且使得它们在 Hadoop 运转的机制是有技巧的，所以我们实施了一整套管理增量的 Hadoop 工作流的架构。
由此看来，对于流处理可以有不同的观点。流处理包括了在底层数据处理的时间概念，它不需要数据的静态快照，它可以产生用户可控频率的输出，而不用等待数据集的全部到达。从这个角度上讲，流处理就是广义上的批处理，随着实时数据的流行，会儿更加普遍。
这就是为什么从传统的视角看来流处理是利基应用。我个人认为最大的原因是缺少实时数据收集使得不间断的处理成为了学术性的概念。
我想缺少实时数据收集就像是商用流处理系统注定的命运。他们的客户仍然需要处理面向文件的、每日批量处理 ETL 和数据集成。公司建设流处理系统关注的是提供附着在实时数据流的处理引擎，但是最终当时极少数人真正使用了实时数据流。事实上，在我在 LinkedIn 工作的初期，有一家公司试图把一个非常棒的流处理系统销售给我们，但是因为当时我们的全部数据都按小时收集在的文件里，当时我们提出的最好的应用就是在每小时的最后把这些文件输入到流处理系统中。他们注意到这是一个普遍性的问题。这些异常证明了如下规则：流处理系统要满足的重要商业目标之一是：财务， 它是实时数据流已具备的基准，并且流处理已经成为了瓶颈。
甚至于在一个健康的批处理系统中，流处理作为一种基础架构的实际应用能力是相当广泛的。它跨越了实时数据请求-应答服务和离线批量处理之间的鸿沟。现在的互联网公司，大约 25%的代码可以划分到这个类型中。
最终这些日志解决了流处理中绝大部分关键的技术问题。在我看来，它所解决的最大的问题是它使得多订阅者可以获得实时数据。对这些技术细节感兴趣的朋友，我们可以用开源的 Samza,它是基于这些理念建设的一个流处理系统。这些应用的更多技术细节我们在此文档中有详细的描述。
Data flow graphs 流处理最有趣的角度是它与流处理系统内部无关，但是与之密切相关的是如何扩展了我们谈到的早期数据集成的数据获取的理念。我们主要讨论了基础数据的获取或日志&amp;ndash;事件和各类系统执行中产生的数据等。但是流处理允许我们包括了计算其它数据的数据。这些衍生的数据在消费者看来与他们计算的原始数据没什么差别。这些衍生的数据可以按任意的复杂度进行压缩。
让我们再深入一步。我们的目标是：流处理作业可以读取任意的日志并把日志写入到日志或者其它的系统中。他们用于输入输出的日志把这些处理关联到一组处理过程中。事实上，使用这种样式的集中日志，你可以把组织全部的数据抓取、转化和工作流看成是一系列的日志和写入它们的处理过程。
流处理器根本不需要理想的框架：它可能是读写日志的任何处理器或者处理器集合，但是额外的基础设施和辅助可以提供帮助管理处理代码。
日志集成的目标是双重的：
首先，它确保每个数据集都有多个订阅者和有序的。让我们回顾一下状态复制原则来记住顺序的重要性。为了使这个更加具体，设想一下从数据库中更新数据流&amp;ndash;如果在处理过程中我们把对同一记录的两次更新重新排序，可能会产生错误的输出。 TCP 之类的链接仅仅局限于单一的点对点链接，这一顺序的持久性要优于 TCP 之类的链接，它可以在流程处理失败和重连时仍然存在。
第二，日志提供了流程的缓冲。这是非常基础的。如果处理流程是非同步的，那么上行生成流数据的作业比下行消费流数据的作业运行的更快。这将会导致处理流程阻塞，或者缓冲数据，或者丢弃数据。丢弃数据并不是可行的方法，阻塞将会导致整个流程图立即停止。 日志实际上是一个非常大的缓冲，它允许流程重启或者停止但不会影响流程图其它部分的处理速度。如果要把数据流扩展到更大规模的组织，如果处理作业是由多个不同的团队提供的，这种隔离性是极其重的。我们不能容忍一个错误的作业引发后台的压力，这种压力会使得整个处理流程停止。
Storm和Sama这两者都是按非同步方式设计的，可以使用 Kafka 或者其它类似的系统作为它们的日志。
Stateful Real-Time Processing 一些实时流处理在转化时是无状态的记录。在流处理中大部分的应用会是相当复杂的统计、聚合、不同窗口之间的关联。例如有时人们想扩大包含用户操作信息的事件流（一系列的单击动作）&amp;ndash;实际上关联了用户的单击动作流与用户的账户信息数据库。不变的是这类流程最终会需要由处理器维护的一些状态信息。例如数据统计时，你需要统计到目前为止需要维护的计数器。如果处理器本身失败了，如何正确的维护这些状态信息呢？
最简单的替换方案是把这些状态信息保存在内存中。但是如果流程崩溃，它就会丢失中间状态。如果状态是按窗口维护的，流程就会回退到日志中窗口开始的时间点上。但是，如果统计是按小时进行的，那么这种方式就会变得不可行。
另一个替换方案是简单的存储所有的状态信息到远程的存储系统，通过网络与这些存储关联起来。这种机制的问题是没有本地数据和大量的网络间通信。
我们如何支持处理过程可以像表一样分区的数据呢?
回顾一下关于表和日志二相性的讨论。这一机制提供了工具把数据流转化为与处理过程协同定位的表，同时也提供了这些表的容错处理的机制。
流处理器可以把它的状态保存在本地的表或索引&amp;ndash;bdb,或者leveldb,甚至于类似于 Lucene 或 fastbit 一样不常见的索引。这些内容存储在它的输入流中（或许是使用任意的转化）。生成的变更日志记录了本地的索引，它允许存储事件崩溃、重启等的状态信息。流处理提供了通用的机制用于在本地输入流数据的随机索引中保存共同分片的状态。
当流程运行失败时，它会从变更日志中恢复它的索引。每次备份时，日志把本地状态转化成一系列的增量记录。
这种状态管理的方法有一个优势是把处理器的状态也做为日志进行维护。我们可以把这些日志看成与数据库表相对应的变更日志。事实上，这些处理器同时维护着像共同分片表一样的表。因为这些状态它本身就是日志，其它的处理器可以订阅它。如果流程处理的目标是更新结点的最后状态，这种状态又是流程的输出，那么这种方法就显得尤为重要。
为了数据集成，与来自数据库的日志关联，日志和数据库表的二象性就更加清晰了。变更日志可以从数据库中抽取出来，日志可以由不同的流处理器（流处理器用于关联不同的事件流）按不同的方式进行索引。
我们可以列举在 Samza 中有状态流处理管理的更多细节和大量实用的例子。
Log Compaction 当然，我们不能奢望保存全部变更的完整日志。除非想要使用无限空间，日志不可能完全清除。为了澄清它，我们再来聊聊 Kafka 的实现。在 Kafka 中,清理有两种选择，这取决于数据是否包括关键更新和事件数据。对于事件数据，Kafka 支持仅维护一个窗口的数据。通常，配置需要一些时间，窗口可以按时间或空间定义。虽然对于关键数据而言，完整日志的重要特征是你可以重现源系统的状态信息，或者在其它的系统重现。
随着时间的推移，保持完整的日志会使用越来越多的空间，重现所耗费的时间越来越长。因些在 Kafka 中,我们支持不同类型的保留。我们移除了废弃的记录(这些记录的主键最近更新过)而不是简单的丢弃旧日志。我们仍然保证日志包含了源系统的完整备份，但是现在我们不再重现原系统的全部状态，而是仅仅重现最近的状态。我们把这一特征称为日志压缩。
Part Four: System Building 我们最后要讨论的是在线数据系统设计中日志的角色。
在分布式数据库数据流中日志的角色和在大型组织机构数据完整中日志的角色是相似的。在这两个应用场景中，日志是对于数据源是可靠的，一致的和可恢复的。组织如果不是一个复杂的分布式数据系统呢，它究竟是什么？
分类计价吗？(Unbundling) 如果换个角度，你可以看到把整个组织系统和数据流看做是单一的分布式数据系统。你可以把所有的子查询系统（诸如 Redis, SOLR,Hive 表等）看成是数据的特定索引。你可以把 Storm 或 Samza 一样的流处理系统看成是发展良好的触发器和视图具体化机制。我已经注意到，传统的数据库管理人员非常喜欢这样的视图，因为它最终解释了这些不同的数据系统到底是做什么用的&amp;ndash;它们只是不同的索引类型而已。
不可否认这类数据库系统现在大量的出现，但是事实上，这种复杂性一直都存在。即使是在关系数据库系统的鼎盛时期，组织中有大量的关系数据库系统。或许自大型机时代开始，所有的数据都存储在相同的位置，真正的集成是根本不存在的。存在多种外在需求，需要把数据分解成多个系统，这些外在需求包括：规模、地理因素、安全性，性能隔离是最常见的因素。这些需求都可以由一个优质的系统实现：例如，组织可以使用单一的 Hadoop 聚簇，它包括了全部的数据，可以服务于大型的和多样性的客户。
因此在向分布式系统变迁的过程中，已经存在一种处理数据的简便的方法：把大量的不同系统的小的实例聚合成为大的聚簇。许多的系统还不足以支持这一方法：因为它们不够安全，或者性能隔离性得不到保证，或者规模不符合要求。不过这些问题都是可以解决的。
依我之见，不同系统大量出现的原因是建设分布式数据库系统很困难。通过削减到单一的查询或者用例，每个系统都可以把规模控制到易于实现的程度。但是运行这些系统产生的复杂度依然很高。
未来这类问题可能的发展趋势有三种：
第一种可能是保持现状：孤立的系统还会或长或短的持续一段时间。这是因为建设分布式系统的困难很难克服，或者因为孤立系统的独特性和便捷性很难达到。基于这些原因，数据集成的核心问题仍然是如何恰当的使用数据。因此，集成数据的外部日志非常的重要。
第二种可能是重构：具备通用性的单一的系统逐步融合多个功能形成超极系统。这个超级系统表面看起来类似关系数据库系统，但是在组织中你使用时最大的不同是你只需要一个大的系统而不是无数个小系统。在这个世界里，除了在系统内已解决的这个问题不存在什么真正的数据集成问题。我想这是因为建设这样的系统的实际困难。
虽然另一种可能的结果对于工程师来说是很有吸引力的。新一代数据库系统的特征之一是它们是完全开源的。开源提供了一种可能性：数据基础架构不必打包成服务集或者面向应用的系统接口。在 Java 栈中，你可以看到在一定程度上，这种状况已经发生了。
 Zookeeper用于处理多个系统之间的协调，或许会从诸如Helix 或者Curator等高级别的抽象中得到一些帮助。 Mesos和YARN用于处理流程可视化和资源管理。 Lucene和LevelDB等嵌入式类库做为索引。 Netty,Jetty和Finagle,rest.li等封装成高级别的用于处理远程通信。 Avro,Protocol Buffers,Thrift和umpteen zillion等其它类库用于处理序列化。 Kafka和Bookeeper提供支持日志。  如果你把这些堆放在一起，换个角度看，它有点像是简化版的分布式数据库系统工程。你可以把这些拼装在一起，创建大量的可能的系统。显而易见，现在探讨的不是最终用户所关心的 API 或者如何实现，而是在不断多样化和模块化的过程中如何设计实现单一系统的途径。因为随着可靠的、灵活的模块的出现，实施分布式系统的时间周期由年缩减为周，聚合形成大型整体系统的压力逐步消失。
The place of the log in system architecture 那些提供外部日志的系统如今已允许个人电脑抛弃他们自身复杂的日志系统转而使用共享日志。在我看来，日志可以做到以下事情：
 通过对节点的并发更新的排序处理数据的一致性（无论在及时还是最终情况下） 提供节点之间的数据复制 提供”commit“语法（只有当写入器确保数据不会丢失时才会写入） 位系统提供外部的数据订阅资源 提供存储失败的复制操作和引导新的复制操作的能力 处理节点间的数据平衡  这实际上是一个数据分发系统最重要的部分，剩下的大部分内容与终端调用的 API 和索引策略相关。这正是不同系统间的差异所在，例如：一个全文本查询语句需要查询所有的分区，而一个主键查询只需要查询负责键数据的单个节点就可以了。
下面我们来看下该系统是如何工作的。系统被分为两个逻辑区域：日志和服务层。日志按顺序捕获状态变化，服务节点存储索引提供查询服务需要的所有信息（键-值的存储可能以 B-tree 或 SSTable 的方式进行，而搜索系统可能存在与之相反的索引）。写入器可以直接访问日志，尽管需要通过服务层代理。在写入日志的时候会产生逻辑时间戳（即 log 中的索引），如果系统是分段式的，那么就会产生与段数目相同数量的日志文件和服务节点，这里的数量和机器数量可能会有较大差距。
服务节点订阅日志信息并将写入器按照日志存储的顺序尽快应用到它的本地索引上。
客户端只要在查询语句中提供对应的写入器的时间戳，它就可以从任何节点中获取”读写“语义。服务节点收到该查询语句后会将其中的时间戳与自身的索引比较，如果必要，服务节点会延迟请求直到对应时间的索引建立完毕，以免提供旧数据。
服务节点或许根本无需知道”控制“或”lerder 选举（leader election）“的概念，对很多简单的操作，服务节点可以爱完全脱离领导的情况下提供服务，日志即是信息的来源。
分发系统所需要做的其中一个比较复杂的工作，就是修复失败节点并移除几点之间的隔离。保留修复的数据并结合上各区域内的数据快照是一种较为典型的做法，它与保留完整的数据备份并从垃圾箱内回收日志的做法几乎等价。这就使得服务层简单了很多，日志系统也更有针对性。
有了这个日志系统，你可以订阅到API，这个API提供了把ETL提供给其它系统的数据内容。事实上，许多系统都可以共享相同的日志同时提供不同的索引，如下所示：
这个系统的视图可以清晰的分解到日志和查询API,因为它允许你从系统的可用性和一致性角度分解查询的特征。这可以帮助我们对系统进行分解，并理解那些并没按这种方式设计实施的系统。
虽然Kafka和Bookeeper都是一致性日志，但这不是必须的，也没什么意义。你可以轻松的把Dynamo之类的数据构分解为一致性的AP日志和键值对服务层。这样的日志使用起来灵活，因为它重传了旧消息，像Dynamo一样，这样的处理取决于消息的订阅者。
在很多人看来，在日志中另外保存一份数据的完整复本是一种浪费。事实上，虽然有很多因素使得这件事并不困难。首先，日志可以是一种有效的存储机制。我们在Kafka生产环境的服务器上存储了5 TB的数据。同时有许多的服务系统需要更多的内存来提供有效的数据服务，例如文本搜索，它通常是在内存中的。服务系统同样也需样硬盘的优化。例如，我们的实时数据系统或者在内存外提供服务或者使用固态硬盘。相反，日志系统只需要线性的读写，因此，它很乐于使用TB量级的硬盘。最终，如上图所示，由多个系统提供的数据，日志的成本分摊到多个索引上，这种聚合使得外部日志的成本降到了最低点。
LinkedIn就是使用了这种方式实现它的多个实时查询系统的。这些系统提供了一个数据库（使用数据总线做为日志摘要，或者从Kafka去掉专用的日志），这些系统在顶层数据流上还提供了特殊的分片、索引和查询功能。这也是我们实施搜索、社交网络和OLAP查询系统的方式。事实上这种方式是相当普遍的：为多个用于实时服务的服务系统提供单一的数据（这些来自Hadoop的数据或是实时的或是衍生的）。这种方式已被证实是相当简洁的。这些系统根本不需要外部可写入的API，Kafka和数据库被用做系统的记录和变更流，通过日志你可以查询系统。持有特定分片的结点在本地完成写操作。这些结点盲目的把日志提供的数据转录到它们自己的存储空间中。通过回放上行流日志可以恢复转录失败的结点。
这些系统的程度则取决于日志的多样性。一个完全可靠的系统可以用日志来对数据分片、存储结点、均衡负载，以及用于数据一致性和数据复制等多方面。在这一过程中，服务层实际上只不过是一种缓存机制，这种缓存机制允许直接写入日志的流处理。
结束语 如果你对于本文中所谈到的关于日志的大部内容，如下内容是您可以参考的其它资料。对于同一事务人们会用不同的术语，这会让人有一些困惑，从数据库系统到分布式系统，从各类企业级应用软件到广阔的开源世界。无论如何，在大方向上还是有一些共同之处。
references  参考翻译 部分自己翻译，大段的抄这里的翻译～ 参考文献见此 </content>
    </entry>
    
     <entry>
        <title>Newdb</title>
        <url>/post/db/newdb/</url>
        <categories>
          
        </categories>
        <tags>
          
        </tags>
        <content type="html"> 文章简介：简单介绍最近自己最近设计实现 newdb 的工作
项目地址 anydemo/newdb
最近一段时间花了很大的力气学习数据库的知识，设计实现了简单的 RDBMS, 收获比较丰富
开始先从数据库底层的数据持久化开始。再此设计的数据库主要是固定类型大小的关系型数据库。一个 table 中有固定大小的一定数量的数据列，每一个 tuple 的大小是固定不变的，也就是每一个 page 的可以容纳的 tuple 是一定的，这样很容易在记录某一个 tuple 的位置、以及其在文件中的位置。在这过程中收获最大的部分就是对于 Marshal 于 Unmarshal 的理解更进一步，知道数据如何在数据库中是如何存储的，以及事务的本质。
最后通过一个 Sequences Scan 的例子，手写一个物理执行计划，执行一次全表扫描。
未来还有很多的内容可以做，暂时告一段落，最近抽时间沉淀一下，思考一下未来，回头有时间了再回头继续加新的内容。毕竟issues积攒了太多。
</content>
    </entry>
    
     <entry>
        <title>Go 启动多个程序，及 IPC 和 RPC 交互例子,以及 Gracefully Shutdown</title>
        <url>/post/go/rpc-ipc/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>go</tag><tag>IPC</tag><tag>RPC</tag>
        </tags>
        <content type="html"> 文章简介：Go 程序启动 RPC 子进程，通过 pipe 进行交互，以及通过 RPC 交互
代码见 exfly/go-ipc
引言 为什么要写这篇文章。最近看了一些 Docker 源码。dockerd 的架构长这个样子。 一条 docker 命令的执行，比如docker run，是先由 containerd 执行，containerd 也同样不是真正运行容器，他会将执行请求发给 runc，有 runc 真正去执行。dockerd、containderd、runc 分别是三个可执行文件，他们是通过 管道（IPC）以及 rest、RPC 进行交互的。
为了展示三者交互方式是如何进行的，这里写一个简单的 demo 来解释。
正文 RPC 首先说一下 RPC。Go 标准库便有net/rpc，写一个 Go 的 rpc 很简单：
package main import ( &amp;#34;log&amp;#34; &amp;#34;net&amp;#34; &amp;#34;net/http&amp;#34; &amp;#34;net/rpc&amp;#34; ) type Task []string type Todo {ID string} func (t Task) Get(id string, reply *Todo) error { *reply=Todo{ID:id} return nil } func main() { task := new(Task) // Publish the receivers methods 	err := rpc.Register(task) if err != nil { log.Fatal(&amp;#34;Format of service Task isn&amp;#39;t correct. &amp;#34;, err) } // Register a HTTP handler 	rpc.HandleHTTP() listener, e := net.Listen(&amp;#34;unix&amp;#34;, &amp;#34;rpc.sock&amp;#34;) if e != nil { log.Fatal(&amp;#34;Listen error: &amp;#34;, e) } err = http.Serve(listener, nil) if err != nil { log.Fatal(&amp;#34;Error serving: &amp;#34;, err) } }// client client, err := rpc.DialHTTP(&amp;#34;unix&amp;#34;, &amp;#34;rpc.sock&amp;#34;) reply := TODO{} err = client.Call(&amp;#34;Task.Get&amp;#34;, &amp;#34;new_id&amp;#34;, &amp;amp;reply) 如何进行进程间通信 IPC 呢 首先 linux 下的 FIFO（有名管道）是什么样子的
mkfifo tpipe ll # total 8 # drwx------ 2 vagrant vagrant 4096 May 18 17:28 ./ # drwxrwxrwt 10 root root 4096 May 18 17:28 ../ # prw-rw-r-- 1 vagrant vagrant 0 May 18 17:28 tpipe| # 现在一个terminal cat tpipe # 另一个terminal echo tttttttttt &amp;gt; tpipe 此时第一个 terminal 会输出 tttttttttt，另一个命令行会返回
在 Go 中应该如何使用
rpcSvrCmd := exec.Command(conf.RPCSvrBinPath) rpcSvrStdinPipe, err := rpcSvrCmd.StdinPipe() rpcSvrStdoutPipe, err := rpcSvrCmd.StdoutPipe() rpcSvrStderrPipe, err := rpcSvrCmd.StderrPipe() rpcSvrCmd.Start() 如此即可获得子进程的各种 FIFO
完整例子 代码见 exfly/go-ipc
make task rpc &amp;amp;&amp;amp; ./bin/task # ▶ running gofmt… # ▶ running golint… # ▶ building executable… # ▶ building executable… # 2019/05/19 01:33:12 sleep 1s to wait rpc server startup # 2019/05/19 01:33:13 2019/05/19 01:33:12 Serving RPC server on {unix rpc.sock} # 2019/05/19 01:33:13 Finish App: {Finish App Started} # 2019/05/19 01:33:13 2019/05/19 01:33:13 stop the server # 2019/05/19 01:33:13 2019/05/19 01:33:13 deleted socket file: rpc.sock # 2019/05/19 01:33:13 pipe rpc_srv_stderr has Closed # 2019/05/19 01:33:13 pipe rpc_srv_stdout has Closed # 2019/05/19 01:33:13 stop subproc ./bin/task-rpc success 当前更新主要以发布代码为主，没有详细解释，详细见代码 exfly/go-ipc
</content>
    </entry>
    
     <entry>
        <title>Docker 源码阅读: Docker Volume</title>
        <url>/post/docker/docker-volume/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>docker</tag><tag>源码</tag>
        </tags>
        <content type="html"> 文章简介：描述 docker volume 的原理
docker 存储基于 UnionFS 实现，所有容器存储是将多个 layout 层通过类似 aufs 或者 overlay2 将多个层联合到一起，mount 成新的目录，在通过 namespaces 将新的文件目录 chroot 进入新启动的进程，达到文件隔离的目的。
 对于读请求，由于写实复制技术，会直接读取底层文件 对于写请求，会先将文件复制到读写层，然后进行修改文件 对于删除，没有真正删除文件，只是讲文件标记删除，没有真正删除下层文件。  命令  docker volume create volume-for-test可以创建新的 volume
 docker volume ls 查看新创建的 volume
 docker volume inspect volume-for-test
 docker run -d --name devtest --mount source=volume-for-test,target=/app alpine /bin/sh
 docker inspect &amp;lt;container_id&amp;gt;| grap Mounts 可以看到刚刚 mount 进去的 volume /var/lib/docker/volumes/volume-for-test/_data
  related links  Manage data in Docker Use volumes docker 存储基础 </content>
    </entry>
    
     <entry>
        <title>Docker 源码阅读: Docker Build</title>
        <url>/post/docker/docker-build/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>docker</tag><tag>源码</tag>
        </tags>
        <content type="html"> 文章简介：docker build 源码阅读
阅读源码顺序  api/server/router/build/build_routes.go#postBuild api/server/backend/build/backend.go#Build builder/builder-next/builder.go#Build builder/dockerfile/builder.go#Build builder/dockerfile/builder.go#dispatchDockerfileWithCancellation builder/dockerfile/evaluator.go#dispatch，在这里，有所有命令的执行方式，可以仔细研究一下  在dispatch中有 dockerfile 支持的指令，如RUN等。以 RUN 为例，dockerd 会读取 CLI 发来的 dockerfile，解析后。如果为 RUN，则会启动一个新的 container,然后在容器中执行，执行结束后将当前层 commit，继续执行下一个指令.
其他  从 v18.09开始，docker build 依赖于 buildkit </content>
    </entry>
    
     <entry>
        <title>Docker 源码阅读: Docker Run</title>
        <url>/post/docker/docker-run/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>docker</tag><tag>源码</tag>
        </tags>
        <content type="html"> 文章简介：通过分析 docker run 命令，理解 Docker 的工作原理
docker run执行流程分析 整体执行流程: 从本地镜像中寻找是否存在命令指定镜像，如果存在则正常返回，执行接下来流程。如果不存在镜像，deamon 返回错误，由 cli 重新发起 pull 请求，之后重试。
总体描述  createContainer cli 通过 rest 接口请求 dockerd 创建容器，dockerd 设置基础配置后记录创建的容器 ContainerStart cli 通过 rest 接口请求 dockerd 运行容器，dockerd 使用 rpc 链接 containnerd 进行运行容器  代码 lanything/code-read forked moby/moby, lanything/cli forked docker/cli
createContainer  docker/cli cli/command/container/run.go#NewRunCommand docker/cli cli/command/container/run.go#runContainer  // cli/command/container/create.go#createContainer func createContainer(ctx context.Context, dockerCli command.Cli, containerConfig *containerConfig, opts *createOptions) (*container.ContainerCreateCreatedBody, error) { config := containerConfig.Config hostConfig := containerConfig.HostConfig networkingConfig := containerConfig.NetworkingConfig stderr := dockerCli.Err() warnOnOomKillDisable(*hostConfig, stderr) warnOnLocalhostDNS(*hostConfig, stderr) var ( trustedRef reference.Canonical namedRef reference.Named ) containerIDFile, err := newCIDFile(hostConfig.ContainerIDFile) if err != nil { return nil, err } defer containerIDFile.Close() ref, err := reference.ParseAnyReference(config.Image) if err != nil { return nil, err } if named, ok := ref.(reference.Named); ok { namedRef = reference.TagNameOnly(named) if taggedRef, ok := namedRef.(reference.NamedTagged); ok &amp;amp;&amp;amp; !opts.untrusted { var err error trustedRef, err = image.TrustedReference(ctx, dockerCli, taggedRef, nil) if err != nil { return nil, err } config.Image = reference.FamiliarString(trustedRef) } } //create the container 	response, err := dockerCli.Client().ContainerCreate(ctx, config, hostConfig, networkingConfig, opts.name) //if image not found try to pull it 	if err != nil { if apiclient.IsErrNotFound(err) &amp;amp;&amp;amp; namedRef != nil { fmt.Fprintf(stderr, &amp;#34;Unable to find image &amp;#39;%s&amp;#39; locally\n&amp;#34;, reference.FamiliarString(namedRef)) // we don&amp;#39;t want to write to stdout anything apart from container.ID 	if err := pullImage(ctx, dockerCli, config.Image, opts.platform, stderr); err != nil { return nil, err } if taggedRef, ok := namedRef.(reference.NamedTagged); ok &amp;amp;&amp;amp; trustedRef != nil { if err := image.TagTrusted(ctx, dockerCli, trustedRef, taggedRef); err != nil { return nil, err } } // Retry 	var retryErr error response, retryErr = dockerCli.Client().ContainerCreate(ctx, config, hostConfig, networkingConfig, opts.name) if retryErr != nil { return nil, retryErr } } else { return nil, err } } for _, warning := range response.Warnings { fmt.Fprintf(stderr, &amp;#34;WARNING: %s\n&amp;#34;, warning) } err = containerIDFile.Write(response.ID) return &amp;amp;response, err }  docker/cli cli/command/container/create.go#createContainer 调用 ContainerCreate moby/moby api/server/router/container/container.go#initRoutes.postContainersCreate moby/moby api/server/router/container/container_routes.go#postContainersCreate moby/moby daemon/create.go#ContainerCreate  containerCreate // `moby/moby` daemon/create.go#containerCreate func (daemon *Daemon) containerCreate(opts createOpts) (containertypes.ContainerCreateCreatedBody, error) { start := time.Now() if opts.params.Config == nil { return containertypes.ContainerCreateCreatedBody{}, errdefs.InvalidParameter(errors.New(&amp;#34;Config cannot be empty in order to create a container&amp;#34;)) } os := runtime.GOOS if opts.params.Config.Image != &amp;#34;&amp;#34; { img, err := daemon.imageService.GetImage(opts.params.Config.Image) if err == nil { os = img.OS } } else { // This mean scratch. On Windows, we can safely assume that this is a linux 	// container. On other platforms, it&amp;#39;s the host OS (which it already is) 	if runtime.GOOS == &amp;#34;windows&amp;#34; &amp;amp;&amp;amp; system.LCOWSupported() { os = &amp;#34;linux&amp;#34; } } warnings, err := daemon.verifyContainerSettings(os, opts.params.HostConfig, opts.params.Config, false) if err != nil { return containertypes.ContainerCreateCreatedBody{Warnings: warnings}, errdefs.InvalidParameter(err) } err = verifyNetworkingConfig(opts.params.NetworkingConfig) if err != nil { return containertypes.ContainerCreateCreatedBody{Warnings: warnings}, errdefs.InvalidParameter(err) } if opts.params.HostConfig == nil { opts.params.HostConfig = &amp;amp;containertypes.HostConfig{} } err = daemon.adaptContainerSettings(opts.params.HostConfig, opts.params.AdjustCPUShares) if err != nil { return containertypes.ContainerCreateCreatedBody{Warnings: warnings}, errdefs.InvalidParameter(err) } container, err := daemon.create(opts) if err != nil { return containertypes.ContainerCreateCreatedBody{Warnings: warnings}, err } containerActions.WithValues(&amp;#34;create&amp;#34;).UpdateSince(start) if warnings == nil { warnings = make([]string, 0) // Create an empty slice to avoid https://github.com/moby/moby/issues/38222 	} return containertypes.ContainerCreateCreatedBody{ID: container.ID, Warnings: warnings}, nil } GetImages // GetImage returns an image corresponding to the image referred to by refOrID. func (i *ImageService) GetImage(refOrID string) (*image.Image, error) { ref, err := reference.ParseAnyReference(refOrID) if err != nil { return nil, errdefs.InvalidParameter(err) } namedRef, ok := ref.(reference.Named) if !ok { digested, ok := ref.(reference.Digested) if !ok { return nil, ErrImageDoesNotExist{ref} } id := image.IDFromDigest(digested.Digest()) if img, err := i.imageStore.Get(id); err == nil { return img, nil } return nil, ErrImageDoesNotExist{ref} } if digest, err := i.referenceStore.Get(namedRef); err == nil { // Search the image stores to get the operating system, defaulting to host OS. 	id := image.IDFromDigest(digest) if img, err := i.imageStore.Get(id); err == nil { return img, nil } } // Search based on ID 	if id, err := i.imageStore.Search(refOrID); err == nil { img, err := i.imageStore.Get(id) if err != nil { return nil, ErrImageDoesNotExist{ref} } return img, nil } return nil, ErrImageDoesNotExist{ref} } ImageService.imageStore 在mobydaemon/daemon.go#NewDaemon 中有设置 ImageService.imageStore 在mobyimage/store.go#NewImageStore 有初始化
ContainerStart moby daemon/start.go#ContainerStart
func (daemon *Daemon) ContainerStart(name string, hostConfig *containertypes.HostConfig, checkpoint string, checkpointDir string) error { if checkpoint != &amp;#34;&amp;#34; &amp;amp;&amp;amp; !daemon.HasExperimental() { return errdefs.InvalidParameter(errors.New(&amp;#34;checkpoint is only supported in experimental mode&amp;#34;)) } container, err := daemon.GetContainer(name) if err != nil { return err } validateState := func() error { container.Lock() defer container.Unlock() if container.Paused { return errdefs.Conflict(errors.New(&amp;#34;cannot start a paused container, try unpause instead&amp;#34;)) } if container.Running { return containerNotModifiedError{running: true} } if container.RemovalInProgress || container.Dead { return errdefs.Conflict(errors.New(&amp;#34;container is marked for removal and cannot be started&amp;#34;)) } return nil } if err := validateState(); err != nil { return err } // Windows does not have the backwards compatibility issue here. 	if runtime.GOOS != &amp;#34;windows&amp;#34; { // This is kept for backward compatibility - hostconfig should be passed when 	// creating a container, not during start. 	if hostConfig != nil { logrus.Warn(&amp;#34;DEPRECATED: Setting host configuration options when the container starts is deprecated and has been removed in Docker 1.12&amp;#34;) oldNetworkMode := container.HostConfig.NetworkMode if err := daemon.setSecurityOptions(container, hostConfig); err != nil { return errdefs.InvalidParameter(err) } if err := daemon.mergeAndVerifyLogConfig(&amp;amp;hostConfig.LogConfig); err != nil { return errdefs.InvalidParameter(err) } if err := daemon.setHostConfig(container, hostConfig); err != nil { return errdefs.InvalidParameter(err) } newNetworkMode := container.HostConfig.NetworkMode if string(oldNetworkMode) != string(newNetworkMode) { // if user has change the network mode on starting, clean up the 	// old networks. It is a deprecated feature and has been removed in Docker 1.12 	container.NetworkSettings.Networks = nil if err := container.CheckpointTo(daemon.containersReplica); err != nil { return errdefs.System(err) } } container.InitDNSHostConfig() } } else { if hostConfig != nil { return errdefs.InvalidParameter(errors.New(&amp;#34;Supplying a hostconfig on start is not supported. It should be supplied on create&amp;#34;)) } } // check if hostConfig is in line with the current system settings. 	// It may happen cgroups are umounted or the like. 	if _, err = daemon.verifyContainerSettings(container.OS, container.HostConfig, nil, false); err != nil { return errdefs.InvalidParameter(err) } // Adapt for old containers in case we have updates in this function and 	// old containers never have chance to call the new function in create stage. 	if hostConfig != nil { if err := daemon.adaptContainerSettings(container.HostConfig, false); err != nil { return errdefs.InvalidParameter(err) } } return daemon.containerStart(container, checkpoint, checkpointDir, true) } moby daemon/start.go#containerStart
// containerStart prepares the container to run by setting up everything the // container needs, such as storage and networking, as well as links // between containers. The container is left waiting for a signal to // begin running. func (daemon *Daemon) containerStart(container *container.Container, checkpoint string, checkpointDir string, resetRestartManager bool) (err error) { start := time.Now() container.Lock() defer container.Unlock() if resetRestartManager &amp;amp;&amp;amp; container.Running { // skip this check if already in restarting step and resetRestartManager==false 	return nil } if container.RemovalInProgress || container.Dead { return errdefs.Conflict(errors.New(&amp;#34;container is marked for removal and cannot be started&amp;#34;)) } if checkpointDir != &amp;#34;&amp;#34; { // TODO(mlaventure): how would we support that? 	return errdefs.Forbidden(errors.New(&amp;#34;custom checkpointdir is not supported&amp;#34;)) } // if we encounter an error during start we need to ensure that any other 	// setup has been cleaned up properly 	defer func() { if err != nil { container.SetError(err) // if no one else has set it, make sure we don&amp;#39;t leave it at zero 	if container.ExitCode() == 0 { container.SetExitCode(128) } if err := container.CheckpointTo(daemon.containersReplica); err != nil { logrus.Errorf(&amp;#34;%s: failed saving state on start failure: %v&amp;#34;, container.ID, err) } container.Reset(false) daemon.Cleanup(container) // if containers AutoRemove flag is set, remove it after clean up 	if container.HostConfig.AutoRemove { container.Unlock() if err := daemon.ContainerRm(container.ID, &amp;amp;types.ContainerRmConfig{ForceRemove: true, RemoveVolume: true}); err != nil { logrus.Errorf(&amp;#34;can&amp;#39;t remove container %s: %v&amp;#34;, container.ID, err) } container.Lock() } } }() if err := daemon.conditionalMountOnStart(container); err != nil { return err } if err := daemon.initializeNetworking(container); err != nil { return err } spec, err := daemon.createSpec(container) if err != nil { return errdefs.System(err) } if resetRestartManager { container.ResetRestartManager(true) container.HasBeenManuallyStopped = false } if daemon.saveApparmorConfig(container); err != nil { return err } if checkpoint != &amp;#34;&amp;#34; { checkpointDir, err = getCheckpointDir(checkpointDir, checkpoint, container.Name, container.ID, container.CheckpointDir(), false) if err != nil { return err } } createOptions, err := daemon.getLibcontainerdCreateOptions(container) if err != nil { return err } ctx := context.TODO() err = daemon.containerd.Create(ctx, container.ID, spec, createOptions) // daemon/daemon.go#NewDaemon#L1043 libcontainerd.NewClient 	if err != nil { if errdefs.IsConflict(err) { logrus.WithError(err).WithField(&amp;#34;container&amp;#34;, container.ID).Error(&amp;#34;Container not cleaned up from containerd from previous run&amp;#34;) // best effort to clean up old container object 	daemon.containerd.DeleteTask(ctx, container.ID) if err := daemon.containerd.Delete(ctx, container.ID); err != nil &amp;amp;&amp;amp; !errdefs.IsNotFound(err) { logrus.WithError(err).WithField(&amp;#34;container&amp;#34;, container.ID).Error(&amp;#34;Error cleaning up stale containerd container object&amp;#34;) } err = daemon.containerd.Create(ctx, container.ID, spec, createOptions) } if err != nil { return translateContainerdStartErr(container.Path, container.SetExitCode, err) } } // TODO(mlaventure): we need to specify checkpoint options here 	pid, err := daemon.containerd.Start(context.Background(), container.ID, checkpointDir, container.StreamConfig.Stdin() != nil || container.Config.Tty, container.InitializeStdio) if err != nil { if err := daemon.containerd.Delete(context.Background(), container.ID); err != nil { logrus.WithError(err).WithField(&amp;#34;container&amp;#34;, container.ID). Error(&amp;#34;failed to delete failed start container&amp;#34;) } return translateContainerdStartErr(container.Path, container.SetExitCode, err) } container.SetRunning(pid, true) container.HasBeenStartedBefore = true daemon.setStateCounter(container) daemon.initHealthMonitor(container) if err := container.CheckpointTo(daemon.containersReplica); err != nil { logrus.WithError(err).WithField(&amp;#34;container&amp;#34;, container.ID). Errorf(&amp;#34;failed to store container&amp;#34;) } daemon.LogContainerEvent(container, &amp;#34;start&amp;#34;) containerActions.WithValues(&amp;#34;start&amp;#34;).UpdateSince(start) return nil } 上边这段代码到 err = daemon.containerd.Create(ctx, container.ID, spec, createOptions) // daemon/daemon.go#NewDaemon#L1043 libcontainerd.NewClient 这里会比较麻烦，需要到libcontainerd/libcontainerd_linux.go#NewClient寻找源码
// libcontainerd/remote/client.go#Creat( func (c *client) Create(ctx context.Context, id string, ociSpec *specs.Spec, runtimeOptions interface{}) error { bdir := c.bundleDir(id) c.logger.WithField(&amp;#34;bundle&amp;#34;, bdir).WithField(&amp;#34;root&amp;#34;, ociSpec.Root.Path).Debug(&amp;#34;bundle dir created&amp;#34;) _, err := c.client.NewContainer(ctx, id, containerd.WithSpec(ociSpec), containerd.WithRuntime(runtimeName, runtimeOptions), WithBundle(bdir, ociSpec), ) if err != nil { if containerderrors.IsAlreadyExists(err) { return errors.WithStack(errdefs.Conflict(errors.New(&amp;#34;id already in use&amp;#34;))) } return wrapError(err) } return nil } 技巧 在这过程中，比较方便的寻找代码实现方法是 比如 ContainerExecStart 是一个接口中的方法，希望找到她的实现可以通过比较简单的搜索注释寻找到方法：比如搜索 //ContainerExecStart
</content>
    </entry>
    
     <entry>
        <title>Docker 源码阅读: 开发环境搭建</title>
        <url>/post/docker/docker-source-dev-install/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>docker</tag><tag>container</tag><tag>源码</tag>
        </tags>
        <content type="html"> 文章简介：介绍 docker 开发环境搭建
docker 官方的贡献引导: moby/docs/contributing/README.md
步骤 moby/docs/contributing/set-up-dev-env.md完整的描述了开发环境如何搭建
开发环境搭建  因为网络问题，需要尽快的提速安装 dev 环境，需要配置 Dockerfile 中的配置 APT_MIRROR=mirrors.163.com 运行make --just-print BIND_DIR=. shell简单的看一下 make 都做了那些事情  运行make BIND_DIR=. shell开始安装开发环境
  编译 上一节已经进入 docker 中，可以开始编译 dockerd
 hack/make.sh binary make installcopy binary to container&amp;rsquo;s /usr/local/bin/ dockerd -D &amp;amp;  日常工作流  修改代码 hack/make.sh binary install-binary  常用目录/命令  docker inspect 查看镜像或容器运行配置信息，GraphDriver 为容器挂载信息 /var/lib/docker/overlay2/&amp;lt;image_id&amp;gt;/{merged, diff, work} /var/lib/docker/containers当前运行中的容器配置信息 /var/lib/docker/image镜像库 /var/lib/docker/volumesvolumes 储存位置  调试 调试 makefile make --just-print BIND_DIR=. shell
调试 shell bash -x hack/make.sh binary
</content>
    </entry>
    
     <entry>
        <title>Docker 源码阅读: Docker架构介绍</title>
        <url>/post/docker/docker-architecture/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>docker</tag><tag>源码</tag>
        </tags>
        <content type="html"> 文章简介：docker基本架构，以及在 moby 开源项目相关的一些组件之间如何协同工作的
Docker? 我们可以使用 Docker 做什么 快速，一致地交付您的应用程序
Docker 允许开发人员使用提供应用程序和服务的本地容器在标准化环境中工作，从而简化了开发生命周期。 容器非常适合持续集成和持续交付（CI / CD）工作流程。
docker 全局架构 CLI 使用 Docker REST API 通过脚本或直接 CLI 命令控制 Docker 守护程序或与 Docker 守护程序交互。 许多其他 Docker 应用程序使用底层 API 和 CLI。
Docker daemon 创建和管理 Docker 对象，例如 images，containers，networks 和 volumes。
The Docker daemon Docker daemon（dockerd）监听 Docker API 请求并管理 Docker 对象，例如 images，containers，networks 和 volumes。 Docker daemon 还可以与其他守护程序通信以管理 Docker 服务。
 Note: 例如 dockerd 通过 rest 接口与 containnerd 进行交互，containerd 与 runc 通过 grpc 进行通信
 The Docker client Docker 客户端（docker）是许多 Docker 用户与 Docker 交互的主要方式。 当您使用诸如 docker run 之类的命令时，客户端会将这些命令发送到 dockerd，dockerd 将其执行。 docker (CLI) 命令调用 REST 风格的 Docker API。 Docker 客户端可以与多个守护进程通信。
 Note: docker 可以通过配置环境变量 DOCKER_HOST或者修改配置变量，或者命令行参数的方式连接 dockerd
 Docker registries Docker registry 存储 Docker 镜像。 Docker Hub 是任何人都可以使用的公共注册中心，Docker 配置为默认在 Docker Hub 上查找 images。 您甚至可以运行自己的私人 registry。 如果您使用 Docker Datacenter（DDC），它包含 Docker Trusted Registry（DTR）。
 Note: 可以通过配置不同的 Docker registry 作为 images 下载源，比如公司内部搭建 registry 私服，在平时 CI/CD 中提高工作速度。
 Docker objects 使用 Docker 时，您正在创建和使用 images，containers，networks 和 volumes，plugins 和其他对象。 本节简要介绍其中一些对象。
IMAGES images 是一个只读模板，其中包含有关创建 Docker 容器的说明。 通常，images 基于另一个 images，并带有一些额外的自定义。 例如，您可以构建基于 ubuntu images 的 images，但安装 Apache Web 服务器和应用程序，以及运行应用程序所需的配置详细信息。
您可以创建自己的 images，也可以只使用其他人创建的 images 并在 registries 中发布。 要构建自己的 images，可以使用简单的语法创建 Dockerfile，以定义创建 images 并运行 images 所需的步骤。 Dockerfile 中的每条指令都在 images 中创建一个 layer。 更改 Dockerfile 并重建 images 时，仅重建已更改的那些层。 与其他虚拟化技术相比，这是使 images 如此轻量，小巧和快速的部分原因。
 Note: 比较详细的工作原理可以看下文 Union file systems部分
 CONTAINERS CONTAINERS 是 images 的可运行实例。 您可以使用 Docker API 或 CLI 创建，启动，停止，移动或删除容器。 您可以将容器连接到一个或多个网络，将存储连接到它，甚至可以根据其当前状态创建新 images。
默认情况下，CONTAINERS 与其他 CONTAINERS 及其主机相对隔离。 您可以控制 CONTAINERS 的网络，存储或其他基础子系统与其他 CONTAINERS 或主机的隔离程度。
CONTAINERS 由其 images 以及您在创建或启动时为其提供的任何配置选项定义。 删除 containers 后，对其状态的任何未存储在持久存储中的更改都将消失。
SERVICES services 允许您跨多个 Docker 守护程序扩展容器，这些守护程序一起作为具有多个管理器和工作程序的群组一起工作。 swarm 的每个成员都是 Docker 守护程序，守护进程都使用 Docker API 进行通信。 服务允许您定义所需的状态，例如在任何给定时间必须可用的服务的副本数。 默认情况下，服务在所有工作节点之间进行负载平衡。 对于消费者来说，Docker 服务似乎是一个单独的应用程序。 Docker Engine 支持 Docker 1.12 及更高版本中的 swarm 模式。
Docker 底层技术 Docker 是用 Go 编写的，它利用 Linux 内核的几个功能来提供其功能。使用到的内核特性包括 namespaces、cgroups、Union file systems
namespaces Docker 使用称为 namespaces 的技术来提供称为容器的隔离工作空间。 运行容器时，Docker 会为该容器创建一组 namespaces。
这些 namespaces 提供了一层隔离。 容器的每个方面都在一个单独的命名空间中运行，其访问权限仅限于该 namespaces。
Docker Engine 在 Linux 上使用以下命名空间：
 pid 命名空间：进程隔离（PID：进程 ID）。 net 命名空间：管理网络接口（NET：Networking）。 ipc 名称空间：管理对 IPC 资源的访问（IPC：进程间通信）。 mnt 名称空间：管理文件系统挂载点（MNT：Mount）。 uts 命名空间：隔离内核和版本标识符。 （悉尼科技大学：Unix 分时系统）。  cgroups Linux 上的 Docker Engine 还依赖于另一种称为控制组（cgroups）的技术。 cgroup 将应用程序限制为特定的资源集。 cgroups 允许 Docker Engine 将可用的硬件资源共享给容器，并可选择强制执行限制和约束。 例如，您可以限制特定容器的可用内存。
Union file systems 联合文件系统或 UnionFS 是通过创建 layers 来操作的文件系统，使它们非常轻量和快速。 Docker Engine 使用 UnionFS 为容器提供构建块。 Docker Engine 可以使用多种 UnionFS 变体，包括 AUFS，btrfs，vfs 和 DeviceMapper。
 Note: Docker 默认为 overylay2
 Container format Docker Engine 将 namespaces，cgroups 和 UnionFS 组合成一个称为容器格式的包装器。 默认容器格式是 libcontainer。 将来，Docker 可以通过与 BSD Jails 或 Solaris Zones 等技术集成来支持其他容器格式。
 Note: 如上部分翻译自 docker overview，同时添加自己本人理解。
 moby 等代码的依赖介绍，相关调用方式，架构 Docker 从一个单一的软件转移到一组独立的组件和项目。
Docker 如何运行容器？  Docker 引擎创建 images， 把 images 传递给 containerd， containerd 调用 containerd-shim， containerd-shim 使用 runC 来运行 container， containerd-shim 允许运行时（在本例中为 runC）在启动容器后退出  这个模型的两个主要好处是  deamon 运行较少的容器 能够在不破坏正在运行的容器的情况下重启或升级引擎  containerd containerd 架构 related link  moby libnetwork containerd runc
 Visualizing Docker Containers and Images
 </content>
    </entry>
    
     <entry>
        <title>Docker 源码阅读: 目录</title>
        <url>/post/docker/docker-contents/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>docker</tag><tag>源码</tag>
        </tags>
        <content type="html"> 文章简介：资料汇总以及源码阅读目录
Contents  Docker 架构介绍 Docker 编译开发环境搭建 Docker Run Docker Build Docker Volume todo docker network  资料汇总  Visualizing Docker Containers and Images容器、镜像可视化 Docker 核心技术与实现原理 docker-debug  related links  Visualizing Docker Containers and Images </content>
    </entry>
    
     <entry>
        <title>Contribute to Opensource</title>
        <url>/post/contribute-to-opensource/</url>
        <categories>
          
        </categories>
        <tags>
          
        </tags>
        <content type="html"> 文章简介：如何参与开源项目。在这里分享一些思路和开源资源。
简介 自从学习计算机开始，很多时候希望自己能够也为 opensource 贡献一些什么。这里会总结一些思路为开源做些什么。
思路 Start Your Open Source Career这里简述了如何参与开源项目。对自己有很多启示。我们在工作学习中也会有一些自己感觉很好的对某个技术问题的解决方式,希望可以分享给大家，或者希望学习新的知识，成为某个工具的核心维护者。 这里会总结一些比较好的参与开源项目的思路。
good first issue or help wanted 很多开源项目的issue中已经标记出很多类似good first issue or help wanted的 label，这些 label 表示新人可以来帮忙。可以通过一些网站找到打相应 label 的项目，这可能成为你贡献开源项目的开端。 或许这些网站可以帮到你(来自 github)：
 开源星期五 - opensourcefriday 统计自己参与的项目，同时推荐如何开始 github-explore github 会为你推荐一些你感兴趣的项目 project-based-learning Curated list of project-based tutorials first timers only 贡献 pr 需要的 git 知识，label 搜索，相关订阅提醒等 codetriage 订阅 github 中的项目，issue 等，方便通知自己感兴趣的项目 issuehub 按照标签搜索项目 pullrequestroulette 检索需要 reviews 的 pr up-for-grabs.net 根据 label 等检索项目 how-to-contribute/#a-checklist-before-you-contribute  思路 构建一些工具 比如构建一些项目模版，比如graphql&#43;mongoboilerplate. 比如编写一些平时可以提高工作效率的工具，alibaba/arthas
成为新的维护者 有很多有价值的项目因为没有维护者渐渐被人遗弃。你是否可以成为新的维护者呢？可以通过邮件、twiter 等联系原作者，成为项目维护者是不是很棒？
创建自己的项目 如果自己有对新的技术问题的解决办法，可以开源出来，分享自己是如何解决的
发布，推广，分享 为了确保每个有需要的人都乐意来找到你的模块，你必须：
 撰写 readme 等  license README 版本徽章 贡献指南 提供ISSUE_TEMPLATE 使用本项目的产品  为项目撰写精心设计的在线网站，和文档，可以使用静态网站工具生成，如vuepress 在 StackOverflow 和 GitHub 等社交媒体中寻找相关问题并贴出你的项目，并解答 将项目发布到汇集开源项目的社区中，如HackerNews、reddit、producthunt、hashnode 参与一些线下分享、讨论会、演讲等中介绍你的项目  链接  octobox 将你的 GitHub 通知转成邮件的形式，这是避免因堆积「太多问题」以至于影响关注重要问题的很好的方法 probot GitHub App 可以自动化和改善你的工作流程 refined-github 浏览器扩展，简化了 GitHub 界面并添加了有用的功能 Start Your Open Source Career [](https://github.com/mattermost/mattermost-server) [](https://github.com/tuvtran/project-based-learning#go) [](https://github.com/MunGell/awesome-for-beginners) https://www.atlassian.com/git/tutorials/merging-vs-rebasing </content>
    </entry>
    
     <entry>
        <title>Openapi</title>
        <url>/post/java/openapi/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>openapi</tag><tag>codegen</tag>
        </tags>
        <content type="html"> swagger and openapi 代码生成和文档自动生成一些体会
说在前面 平时使用gqlgen，一般的 workflow 是先写 graphql 的 schema，然后 code generate 对应的 model 和 api 的实现的空接口，自己对应实现对应的 resolver 即可。使用起来很流程。最近调研一下 java 下类似的工具。找到在 java 下 star 最多的项目graphql-java，但并不是这里讨论的。 这里讨论的是基于 spring 的 openapi 的实现和 code generate 方案。
基于 openapi 的 code generate 方案 首先简单介绍一下 openapi，他是语言无关的 restful 描述语言，可以使用 yaml 进行编写接口文档，通过 generate，生成不同语言的 client 和 server。这里有官方的简介。自己比较关注的语言是 python、java、go、rust，js。
踩到的一些坑，这里简单试用了一下generate java。首先，maven下有对应的openapi的plugin，openapi-generator-maven-plugin, 但每次compile都会进行generate，并不是我所希望的，所以这里使用了基于docker的使用方案。代码见exfly:gorgestar/isn,使用generator.sh生成对应的代码，配置文件可以看generator.json，
我的使用策略是，先修改openapi.yaml，创建或者修改restful api，然后 bash ./generator.sh，生成对应接口默认空实现，之后由我们对应的实现接口即可。
一些自己没有进行操作的方案：
 如何保证api和接口文档一致：可以对应的使用swagger的api-doc json进行转换为yaml，对原本手写的openapi.yaml进行比较，确认文档的一致性 这个版本的generate每次都会将所有的文件覆盖，需要编辑.openapi-generator-ignore，类似gitignore的东西，类比修改即可。被忽略的文件，即使被删除，也不会自动生成对应文件，生成逻辑看起来比较傻。  其他资源  exfly:gorgestar/isn,本文项目代码 swagger raml OpenAPI-Specification openapi-generator openapi generator maven plugin
 spring rest docs
 </content>
    </entry>
    
     <entry>
        <title>WSL(windows subsystem for linux) win子系统</title>
        <url>/post/tools/wsl/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>工具</tag><tag>wsl</tag>
        </tags>
        <content type="html"> win 子系统安装与 cmder&#43;zsh 开发环境搭建
说在前面 这里只展示可以做到什么程度，具体怎么做，网上教程很多，后边会贴出自己感觉比较好的网址
大致操作思路 先在 windows 下打开 win subsystem for linux 功能，之后去 win store 中下载对应的 linux 发行版.之后就是打开对应的 linux 发行版的 bash、配置 zsh 了，详细步骤见这里。具体 zsh 怎么折腾，可以看一下这里
贴一张自己配置之后，使用 zsh 和 tmux 之后的截图 给我的体验是，基本可以满足大部分日常开发工作
其他  win 下的 CDEF 盘被挂载到/mnt下，为了方便使用，可以将他们ln -s /mnt/d $HOME/windir，这样方便自己使用 因为之前安装过 vscode，可以直接使用code filename 打开系统中的文件 类似 jdk 这种需要在 subsystem 中重新安装才可以在 wsl 中使用 图中使用的 Cmder 是非常远古的版本，所以请忽略 cmd 之前的框  链接  Windows10 终端优化方案：Ubuntu 子系统&#43;cmder&#43;oh-my-zsh  </content>
    </entry>
    
     <entry>
        <title>前后端分离架构的Vue环境搭建指南</title>
        <url>/post/frontend/front-end-separation-architecture-environment-construction/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>前后端分离</tag><tag>Vue</tag>
        </tags>
        <content type="html"> 使用 Vue 前后端&#43;Go 后端，基于 webpack 代理转发，配置前后端分离架构开发环境
原文地址
Web 研发模式演变 最近研究一下前后端的开发模式，看到一个很好的入门路径developer-roadmap: frontend backend DevOps,可以看一下，效果还是不错的。
之前看到一个说web 研发演进这里总结一下。
很久之前，前后端的分工是，前端从设计师那里拿到设计图纸，转化静态页面模板，由后端工程师进行数据库设计等一系列设计之后，套前端给的模板。如上也即后端渲染。
但是这样的流程，所有工作的 Block 在后端，想进一步提高研发速度，应该如何分工？到如今给出的答案是基于 Nodejs 的前后端分离架构。这时前后端分工是这样的：
前端的工作  UI 设计 前端路由设计 处理浏览器层的展现逻辑  通过 CSS 渲染样式，通过 JavaScript 添加交互功能，HTML 的生成也可以放在这层，具体看应用场景
后端的工作  业务逻辑和 API 的设计和实现 数据库设计和维护 后端缓存设计  前后端分离下协作体系 前后端分离下的协作方式一般是，前后端各司其职，互不影响。
首先，对于后端来说，后端的主要工作依然是传统的数据库设计、业务逻辑设计，但不需要套模板了，而是为前端提供数据接口
其次，对于前端来说，前端的主要工作是，前端的 ui，以及获取数据，在前端渲染。
工作流程是，先进行 API 设计。前后端一起设计数据接口以及数据返回的格式，现在比较常见的是 json 数据。可以根据接口生成一些 mock 用的 json 数据文件，供前端开发使用。后端根据这个 API 规范实现真正的接口。两端分别并行开发。开发结束时候联调，打通前后端之后进行调试。
具体，可以看一下网易前后端分离实践.
基于 Vue 前后端分离环境搭建 这里对前后端分离 Vue 的开发环境进行演示。思路是，前后端分离，后端可以设置 cookie，前端可以接收到配置的 cookie
node 环境安装 安装方法见这里
Vue 安装 npm config set registry &amp;#39;https://registry.npm.taobao.org&amp;#39; npm install -g @vue/cli vue init webpack demo cd demo npm dev run 即可在浏览器中看到效果，熟悉的 vue 页面
安装 axios，实现前后端交互，并实现后端设置 cookie，在前端可以生效 安装 axios
npm install axios 修改/src/components/HelloWorld.vue 中对应的 srcipt
&amp;lt;script&amp;gt; import axios from &amp;#39;axios&amp;#39; axios.get(&amp;#39;/sc&amp;#39;) // 使用ajax export default { name: &amp;#39;HelloWorld&amp;#39;, data () { return { msg: &amp;#39;Welcome to Your Vue.js App&amp;#39; } } } &amp;lt;/script&amp;gt;  最终完成的时候，访问 &amp;lsquo;/&amp;lsquo;，可以看到 cookie 添加了一个 kv 对。现在暂时看不到效果，因为接口后端没有实现。
后端接口实现,这里使用的 go，具体 go 编译器的安装方法见这里
package main import ( &amp;#34;log&amp;#34; &amp;#34;net/http&amp;#34; &amp;#34;time&amp;#34; ) func LoggingMiddleware(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { log.Println(&amp;#34;-&amp;gt;&amp;#34;, r.URL) next.ServeHTTP(w, r) log.Println(&amp;#34;&amp;lt;-&amp;#34;) }) } func main() { http.Handle(&amp;#34;/sc&amp;#34;, LoggingMiddleware(http.HandlerFunc(indexHandler))) port := &amp;#34;:8081&amp;#34; log.Println(&amp;#34;starting on http://localhost&amp;#34; &#43; port) log.Fatal(http.ListenAndServe(port, nil)) } func indexHandler(w http.ResponseWriter, req *http.Request) { expire := time.Now().AddDate(0, 0, 1) cookie := http.Cookie{Name: &amp;#34;csrftoken&amp;#34;, Value: expire.String(), Expires: expire} http.SetCookie(w, &amp;amp;cookie) } 此时前后端都已经实现了，但是因为前端开在了端口 8080，后端开在了 8081，涉及到跨域，相互没法访问。需要配置一下 webpack 的配置才可以。
// 修改一下文件/config/index.js: 13 line proxyTable: { &amp;#39;/&amp;#39;: { target: &amp;#39;http://localhost:8081&amp;#39;, changeOrigin: true } },  之后，开启前后端服务：
npm dev run go run server.go 之后访问前端页面，localhost:8080，按 F12 -&amp;gt; Application -&amp;gt; Cookies， 即可看到每次刷新都会改变的 csrftoken 的 cookie
最终的网站见ExFly/FrontBackSep
后记 既然在前后端分离后的后端可以配置cookie，其他的所有操作都可以进行了。进一步可以实现其他的操作。如上。
引用  developer-roadmap: frontend backend DevOps web 研发演进 网易前后端分离实践 </content>
    </entry>
    
     <entry>
        <title>技术变革</title>
        <url>/post/architecture/itabstract/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>bigdata</tag><tag>总结</tag>
        </tags>
        <content type="html"> 总结一下数据库、分布式等技术产生的场景的使用场景。包括数据库、缓存、分布式、容器
从使用到的技术来说，如今各大网站平台趋向于使用经过长期检验过的技术，包括不限于动态网站技术、数据库技术、高速缓存技术、负载均衡、分布式相关技术。
首先动态网站技术，开始没有动态网站时期，大部分 BBS 使用基于 Telnet 协议为基础的 BBS 进行交流。HTTP 协议的出现，使得 BS 架构的网站能够展现多媒体等信息如视频、音乐等。起初 HTTP 协议一般仅展示一些静态数据，其表现能力不强，不能更好的动态的为用户交互。之后动态网站技术如 CGI 的出现，使得网站拥有的更加丰富的交互功能。经过数十年的技术积累，动态网站技术已经能够支撑起现如今的大部分信息的获取。
其次是存储技术。开始的时候数据仅仅使用本地文件进行存储。文件的读写效率很高，但是需要程序员重复的使用操作系统提供的较底层的操作来操作文件，对程序员的要求太大。而且对这种结构化的数据来说，可以使用相同的模式进行操作。所以有了数据库关系系统，尤其关系型数据库。从此，对于一些结构化的数据只需要使用 SQL 这种数据操作语言，就可以很方便的操作数据，而真正的数据管理维护工作由数据库管理系统进行维护，人可以通过配置数据库，使数据库获得更好的性能。当用户量很大时候，单机响应出现瓶颈，单台数据库不能够满足这样的请求。可以通过复制的方式，将相同的数据复制到多台机器上，多台机器为用户提供数据，对于读操作进行扩展。对于读多写少的网站，这种方式基本可以实现线性的性能提升。数据量再大一些，使用分库分表方式，把一份数据切片，分布到多台机器上提高性能。进而使用复制与分库分表的方式，进一步压榨单机的性能潜力。单机数据库很强大，但数据量达到一定的数量级，数据库的性能完全不能满足需求。大数据时代，数据量达到 PB、TB 级别。而每一块硬盘仅仅几 T，单机完全不可能将所有的数据都存储下来。如上分库分表的方式基本已经不能完全解决如上的问题，所以出现了分布式数据库，比如 MySQL Cluster 等通过两阶段提交等方式，维护线上数据的强一致性，分布式存储系统，比如 Google 研发的 GFS，现如今正火热的 HDFS 等，基本可以满足海量数据的存储。
高速缓存技术，如一些场景重复读的场景，数据库的查询很昂贵，如何减少数据库的访问次数？使用类似 Redis 的缓存。redis 的数据全部存储在内存中，内存的速度远远超过数据库的查询速度。维护 redis 中的缓存数据一致性，防止缓存穿透、缓存击穿、缓存雪崩的问题，以及如何正确的使用缓存等一系列问题，现如今已经基本有了很好的解决办法。
负载均衡与反向代理，传统中使用 nginx 作为网关，接受的请求分发到多台服务器中，一定程度上分担单台服务器的压力。
对于分布式技术。首先是 Google 研发并使用的 GFS 以及搭建在 GFS 基础上的 BigTable，以及 MapReduce 算法，使得分布式存储与分布式计算成为了可能。对应的开源版本为 HDFS、Hadoop、Spark 等一系列分布式基础设施。存储的分布式，也对应着应用部署的分布式，也就是现如今使用比较广泛的微服务（将一个整体的应用拆分成不同的服务，服务之间分别开发和部署，通过统一的接口进行协作）。微服务部署少则几个，多则成百上千，如此多的服务需要开发部署工作量巨大。同时很常见的问题是开发环境可以正常工作，上线却无法工作。这个问题现如今的解决办法是使用 docker。首先 docker 是一种使用 Linux 内核提供的 cgroup 和 namespace 功能，它可以将计算机的资源进行隔离。使用场景是通过容器编排工具，将服务所依赖的资源通过配置文件进行定义，由编排工具统一对所有的服务进行启动部署。现如今实质上的容器编排标准 Kubernetes，已经被不限于 Google、百度、阿里巴巴、京东等使用。对于 Google 等基本实现容器化、Kubernetes 化。
</content>
    </entry>
    
     <entry>
        <title>Realize代码分析</title>
        <url>/post/tools/realize/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>工具</tag><tag>Go</tag>
        </tags>
        <content type="html"> realize 代码分析
Realize realize 是 Go 写的 workfloaw 工具，可以配置自己的工作流。项目在修改之后需要进行编译、测试，可以还有其他的一系列流程需要走，可以使用 realize 进行自动化。抽点时间研究了一下源码。这里总结一下思路，不是很完整的解释，简单说一下思路。
原理 从 Linux 2.6.13 内核开始，Linux 就推出了 inotify，允许监控程序打开一个独立文件描述符，并针对事件集监控一个或者多个文件，例如打开、关闭、移动/重命名、删除、创建或者改变属性。glib 对对此进行了封装glib/inotify.h,同时各个操作系统都有对应的实现，win 下的 ReadDirectoryChangesW，mac 下的 FSEvents。同时 go 下已经有写好的封装库fsnotify/fsnotify，对不同的平台进行了封装。
简单来讲，内核为应用程序提供了系统级文件修改事件的监视器。当文件进行修改后，会通知应用程序监视的文件已经修改了，之后有realize进行事件的处理即可。
比较有意思的是，yaml文件的marshal和unmarshal。之后可以研究一下。
核心代码 // github.com/oxequa/realize/realize/projects.go func (p *Project) Watch(wg *sync.WaitGroup) { var err error // change channel 	p.stop = make(chan bool) // init a new watcher 	p.watcher, err = NewFileWatcher(p.parent.Settings.Legacy) if err != nil { log.Fatal(err) } defer func() { close(p.stop) p.watcher.Close() }() // before start checks 	p.Before() // start watcher 	go p.Reload(&amp;#34;&amp;#34;, p.stop) L: for { select { case event := &amp;lt;-p.watcher.Events(): if p.parent.Settings.Recovery.Events { log.Println(&amp;#34;File:&amp;#34;, event.Name, &amp;#34;LastFile:&amp;#34;, p.last.file, &amp;#34;Time:&amp;#34;, time.Now(), &amp;#34;LastTime:&amp;#34;, p.last.time) } if time.Now().Truncate(time.Second).After(p.last.time) { // switch event type 	switch event.Op { case fsnotify.Chmod: case fsnotify.Remove: p.watcher.Remove(event.Name) if p.Validate(event.Name, false) &amp;amp;&amp;amp; ext(event.Name) != &amp;#34;&amp;#34; { // stop and restart 	close(p.stop) p.stop = make(chan bool) p.Change(event) go p.Reload(&amp;#34;&amp;#34;, p.stop) } default: if p.Validate(event.Name, true) { fi, err := os.Stat(event.Name) if err != nil { continue } if fi.IsDir() { filepath.Walk(event.Name, p.walk) } else { // stop and restart 	close(p.stop) p.stop = make(chan bool) p.Change(event) go p.Reload(event.Name, p.stop) p.last.time = time.Now().Truncate(time.Second) p.last.file = event.Name } } } } case err := &amp;lt;-p.watcher.Errors(): p.Err(err) case &amp;lt;-p.exit: p.After() break L } } wg.Done() } Reference  用 inotify 监控 Linux 文件系统事件 oxequa/realize fsnotify/fsnotify </content>
    </entry>
    
     <entry>
        <title>SparkStream等相关产品选型以及Spark安装与简单使用</title>
        <url>/post/bigdata/sparkstream/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>BigData</tag><tag>Spark</tag><tag>Stream</tag>
        </tags>
        <content type="html"> 比较SparkStream类似产品如Samza、Storm，介绍Spark和Spark Stream安装和简单使用方法
各产品比较 Samza Samza 是一个分布式的流式数据处理框架（streaming processing），Linkedin 开源的产品， 它是基于 Kafka 消息队列来实现类实时的流式数据处理的。更为准确的说法是，Samza 是通过模块化的形式来使用 Apache Kafka 的，因此可以构架在其他消息队列框架上，但出发点和默认实现是基于 Apache Kafka。
本质上说，Samza 是在消息队列系统上的更高层的抽象，是一种应用流式处理框架在消息队列系统上的一种应用模式的实现。
总的来说，Samza 与 Storm 相比，传输上完全基于 Apache Kafka，集群管理基于 Hadoop YARN，即 Samza 只负责处理这一块具体业务，再加上基于 RocksDB 的状态管理。由于受限于 Kafka 和 YARN，所以它的拓扑结构不够灵活。
Storm Storm 框架与其他大数据解决方案的不同之处，在于它的处理方式。Apcahe Hadoop 本质上来说是一个批处理系统，即目标应用模式是针对离线分析为主。数据被引入Hadoop的分布式文件系统 (HDFS)，并被均匀地分发到各个节点进行处理，HDFS 的数据平衡规则可以参照本文作者发表于IBM的文章《HDFS数据平衡规则及实验介绍》，进行深入了解。当处理完成时，结果数据返回到HDFS，然后可以供处理发起者使用。Storm则支持创建拓扑结构来转换没有终点的数据流。不同于Hadoop作业，这些转换从不会自动停止，它们会持续处理到达的数据，即Storm的流式实时处理方式。
Spark Streaming Spark Streaming 类似于 Apache Storm，用于流式数据的处理。根据其官方文档介绍，Spark Streaming 有高吞吐量和容错能力强这两个特点。Spark Streaming 支持的数据输入源很多，例如：Kafka、Flume、Twitter、ZeroMQ 和简单的 TCP 套接字等等。数据输入后可以用 Spark 的高度抽象原语如：map、reduce、join、window 等进行运算。而结果也能保存在很多地方，如 HDFS，数据库等。另外 Spark Streaming 也能和 MLlib（机器学习）以及 Graphx 完美融合。
在 Spark Streaming中，处理数据的单位是一批而不是单条，而数据采集却是逐条进行的，因此 Spark Streaming系统需要设置间隔使得数据汇总到一定的量后再一并操作，这个间隔就是批处理间隔。批处理间隔（0.2s-2s）是 Spark Streaming 的核心概念和关键参数，它决定了 Spark Streaming 提交作业的频率和数据处理的延迟，同时也影响着数据处理的吞吐量和性能。
Kafka Sreeam Kafka Streams是一个用于处理和分析数据的客户端库。它先把存储在Kafka中的数据进行处理和分析，然后将最终所得的数据结果回写到Kafka或发送到外部系统去。它建立在一些非常重要的流式处理概念之上，例如适当区分事件时间和处理时间、窗口支持，以及应用程序状态的简单（高效）管理。同时，它也基于Kafka中的许多概念，例如通过划分主题进行扩展。此外，由于这个原因，它作为一个轻量级的库可以集成到应用程序中去。这个应用程序可以根据需要独立运行、在应用程序服务器中运行、作为Docker容器，或通过资源管理器（如Mesos）进行操作。
Kafka Sreeam直接解决了流式处理中的很多困难问题:毫秒级延迟的逐个事件处理。有状态的处理，包括分布式连接和聚合。方便的DSL。使用类似DataFlow的模型对无序数据进行窗口化。具有快速故障切换的分布式处理和容错能力。无停机滚动部署。
主要比较Spark Stream和Storm和选择    比较项 SparkStream Storm     血统 UC Berkeley AMP lab Twitter   开源时间 2011.05 2011.09   依赖环境 Java Zookeeper Java Python   开发语言 Scala Java Clojure   支持语言 Scala Java Python R Any   硬盘IO 少 一般   集群支持 超过1000节点 好   吞吐量 好 较好   使用公司 intel 腾讯 淘宝 中移动 Goole 淘宝 百度 Twitter 雅虎   适用场景 较大数据块&amp;amp;需要高时效性的小批量计算 实时小数据块的分析计算   延时 准实时：一次处理一个即将到达的事件 实时：处理在一定的时间内（时间间隔可自己设置）在窗口中收到的一批事件   容错 在批处理级别进行跟踪处理，因此即使发生节点故障等故障，也可以有效地保证每个小批量都能够被精确处理一次 每个单独的记录必须在其通过系统时被跟踪，因此Storm仅保证每个记录至少被处理一次，但是从故障中恢复期间允许出现重复。 这意味着可变状态可能不正确地更新了两次    1.处理模型以及延迟
虽然这两个框架都提供可扩展性(Scalability)和可容错性(Fault Tolerance),但是它们的处理模型从根本上说是不一样的。Storm处理的是每次传入的一个事件，而Spark Streaming是处理某个时间段窗口内的事件流。因此，Storm处理一个事件可以达到亚秒级的延迟，而Spark Streaming则有秒级的延迟。
2.容错和数据保证
在容错数据保证方面的权衡方面，Spark Streaming提供了更好的支持容错状态计算。在Storm中，当每条单独的记录通过系统时必须被跟踪，所以Storm能够至少保证每条记录将被处理一次，但是在从错误中恢复过来时候允许出现重复记录，这意味着可变状态可能不正确地被更新两次。而Spark Streaming只需要在批处理级别对记录进行跟踪处理，因此可以有效地保证每条记录将完全被处理一次，即便一个节点发生故障。虽然Storm的 Trident library库也提供了完全一次处理的功能。但是它依赖于事务更新状态，而这个过程是很慢的，并且通常必须由用户实现。
简而言之,如果你需要亚秒级的延迟，Storm是一个不错的选择，而且没有数据丢失。如果你需要有状态的计算，而且要完全保证每个事件只被处理一次，Spark Streaming则更好。Spark Streaming编程逻辑也可能更容易，因为它类似于批处理程序，特别是在你使用批次(尽管是很小的)时。
3.实现和编程API
Storm主要是由Clojure语言实现，Spark Streaming是由Scala实现。如果你想看看这两个框架是如何实现的或者你想自定义一些东西你就得记住这一点。Storm是由BackType和 Twitter开发，而Spark Streaming是在UC Berkeley开发的。
Storm提供了Java API，同时也支持其他语言的API。 Spark Streaming支持Scala和Java语言(其实也支持Python)。另外Spark Streaming的一个很棒的特性就是它是在Spark框架上运行的。这样你就可以想使用其他批处理代码一样来写Spark Streaming程序，或者是在Spark中交互查询。这就减少了单独编写流批量处理程序和历史数据处理程序。
4.生产支持
Storm已经出现好多年了，而且自从2011年开始就在Twitter内部生产环境中使用，还有其他一些公司。而Spark Streaming是一个新的项目，并且在2013年仅仅被Sharethrough使用(据作者了解)。
Storm是 Hortonworks Hadoop数据平台中流处理的解决方案，而Spark Streaming出现在 MapR的分布式平台和Cloudera的企业数据平台中。除此之外，Databricks是为Spark提供技术支持的公司，包括了Spark Streaming。
5.集群管理集成
尽管两个系统都运行在它们自己的集群上，Storm也能运行在Mesos，而Spark Streaming能运行在YARN 和 Mesos上。
 这里总结了Kafka Stream-Spark Streaming-Storm流式计算框架比较选型的相关资料。
这里由更多的相关产品的差异比较资源：
 Storm介绍 Spark Streaming vs. Kafka Stream 哪个更适合你？ 大数据框架对比：Hadoop、Storm、Samza、Spark和Flink  Spark Streaming与Storm的对比分析 Storm和Spark Streaming的横向比较 Spark Streaming和Storm如何选择？搭建流式实时计算平台，广告日志实时花费 Spark Streaming 新手指南  Spark 介绍 Spark生态 Spark官网简单介绍了spark的的优势。
这里非常详细了介绍Spark生态、各大厂应用场景、Spark基本原理。
Spark 和 Spark Stream的安装和使用 Spark介绍 Spark Streaming 是 Spark Core API 的扩展, 它支持弹性的, 高吞吐的, 容错的实时数据流的处理. 数据可以通过多种数据源获取, 例如 Kafka, Flume, Kinesis 以及 TCP sockets, 也可以通过例如 map, reduce, join, window 等的高级函数组成的复杂算法处理. 最终, 处理后的数据可以输出到文件系统, 数据库以及实时仪表盘中.事实上,你还可以在 data streams（数据流）上使用机器学习以及图计算 算法
在内部, 它工作原理如下, Spark Streaming 接收实时输入数据流并将数据切分成多个 batch（批）数据, 然后由 Spark 引擎处理它们以生成最终的 stream of results in batches（分批流结果）.
Spark Streaming 提供了一个名为 discretized stream 或 DStream 的高级抽象, 它代表一个连续的数据流. DStream 可以从数据源的输入数据流创建, 例如 Kafka, Flume 以及 Kinesis, 或者在其他 DStream 上进行高层次的操作以创建. 在内部, 一个 DStream 是通过一系列的 RDDs 来表示.
你可以使用 Scala , Java 或者 Python（Spark 1.2 版本后引进）来编写 Spark Streaming 程序.
这里是一篇官方编程指南
Spark安装 方式1 wget http://mirror.bit.edu.cn/apache/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz tar -xzf spark-2.3.1-bin-hadoop2.7.tgz # 运行一个例子 cd spark-2.3.1-bin-hadoop2.7 ./bin/run-example SparkPi 方式二 推荐这种方式这里总结了自己搭建各种开发环境的就自动化安装脚本。第一次安装会比较麻烦，之后实现一条命令自动安装。需要vagrant&amp;amp;virtual。有一些依赖docker
git clone https://github.com/ExFly/ComputSciLab.git cd ComputSciLab vagrant up vagrant ssh cd /vagrant/Java source install-small.sh cd /vagrant/Spark ./install.sh cd /vagrant/.softwenv/spark-2.3.1-bin-hadoop2.7 ./bin/run-example SparkPi 结果图：
spark集群 找到一个中文的文档,可以看一下，部署很简单
总结 如上
</content>
    </entry>
    
     <entry>
        <title>分布式数据的最终一致性</title>
        <url>/post/architecture/eventuallyconsistent/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>Architecture</tag><tag>Distributed</tag>
        </tags>
        <content type="html"> 单体应用，需要借助分库分表、复制技术、读写分离提高服务并发访问量。微服务为代表的分布式系统，其高并发和微服务事务一致性该如何保证？
简介 由于自己刚刚接触，自己理解的也不深。在这里，把我整理的一些资料汇总下来。
微服务架构 微服务架构将单应用放在多个相互独立的服务，这个每个服务能够持续独立的开发和部署，难题是数据该如何存储？
多个应用使用同一数据库 传统的单体应用一般采用的是数据库提供的事务一致性，通过数据库提供的提交以及回滚机制来保证相关操作的ACID，这些操作要么同时成功，要么同时失败。各个服务看到数据库中的数据是一致的，同时数据库的操作也是相互隔离的，最后数据也是在数据库中持久存储的。这样的架构不具备横向扩展能力，服务之间的耦合程度也比较高，会存在单点故障。
典型微服务架构 在微服务架构中， 有一个database per service的模式， 这个模式就是每一个服务一个数据库。 这样可以保证微服务独立开发，独立演进，独立部署， 独立团队。
由于一个应用是由一组相互协作的微服务所组成，在分布式环境下由于各个服务访问的数据是相互分离的， 服务之间不能靠数据库来保证事务一致性。 这就需要在应用层面提供一个协调机制，来保证一组事务执行要么成功，要么失败。
CAP定律 一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项。
通过CAP理论，我们知道无法同时满足一致性、可用性和分区容错性这三个特性，那要舍弃哪个呢？
CA without P：如果不要求P（不允许分区），则C（强一致性）和A（可用性）是可以保证的。但其实分区不是你想不想的问题，而是始终会存在，因此CA的系统更多的是允许分区后各子系统依然保持CA。
CP without A：如果不要求A（可用），相当于每个请求都需要在Server之间强一致，而P（分区）会导致同步时间无限延长，如此CP也是可以保证的。很多传统的数据库分布式事务都属于这种模式。
AP wihtout C：要高可用并允许分区，则需放弃一致性。一旦分区发生，节点之间可能会失去联系，为了高可用，每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。现在众多的NoSQL都属于此类。
对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到N个9，即保证P和A，舍弃C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。
对于涉及到钱财这样不能有一丝让步的场景，C必须保证。网络发生故障宁可停止服务，这是保证CA，舍弃P。貌似这几年国内银行业发生了不下10起事故，但影响面不大，报到也不多，广大群众知道的少。还有一种是保证CP，舍弃A。例如网络故障事只读不写。
常用的解决方法 这里总结了一些分布式数据一致性的解决方法。分布式事务保证强一致性，但为了保证数据的一致性，放弃了一些系统性能。另一种保证最终一致性，放弃了时时数据的一致性，但处理效率最好。
这里有一些例子如何解决的。
BASE 这里实验了一个基于BASE协议的最终一致性demo。注意，这里使用到了Kafka，需要自己在本地开Kafka服务。
其他资料  书：大规模分布式存储系统：原理解析与架构实现 书：微服务设计 分布式事务？No, 最终一致性 分布式事务实践 -花钱的，作为目录使用 多研究些架构，少谈些框架（3）&amp;ndash; 微服务和事件驱动 消息中间件（一）分布式系统事务一致性解决方案大对比，谁最好使？ Saga分布式事务解决方案与实践 解决业务代码里的分布式事务一致性问题 分布式事务实践 实战基于Kafka消息驱动最终一致事务（二） </content>
    </entry>
    
     <entry>
        <title>Gradle和Maven使用方法总结</title>
        <url>/post/java/gradle_maven/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>Java</tag>
        </tags>
        <content type="html"> 文章简介： 1.总结gradle和maven正确使用方法 2.开箱即用maven&amp;amp;gradle同时支持的项目配置。
Gradle和Maven使用起来都比较方便，而Gradle使用更灵活，配置更方便。而公司环境一般使用Maven。因此就有了取舍，是迁移到Gradle，还是继续使用Maven？其实不需要纠结，谁说必须取舍的，两个都用起来就是了！！！
说在前面 Gradle和Maven都是项目自动构建工具，编译源代码只是整个过程的一个方面，更重要的是，你要把你的软件发布到生产环境中来产生商业价值，所以，你要运行测试，构建分布、分析代码质量、甚至为不同目标环境提供不同版本，然后部署。整个过程进行自动化操作是很有必要的。
整个过程可以分成以下几个步骤：
 编译源代码 运行单元测试和集成测试 执行静态代码分析、生成分析报告 创建发布版本 部署到目标环境 部署传递过程 执行冒烟测试和自动功能测试  两者都是项目工具，但是maven使用的最多，Gradle是后起之秀，想spring等都是使用gradle构建的。Gradle抛弃了Maven的基于XML的繁琐配置，采用了领域特定语言Groovy的配置，大大简化了构建代码的行数。
比如maven要 这么写
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-core&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;${spring.version}&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; gradle这么写
compile(&amp;#39;org.springframework:spring-core:2.5.6&amp;#39;) 详细的Gradle和Maven比较看这里讲的很好了。gradle官方也对两个工具进行了比较。
我们可以使用其中一个，或者两个一起使用！！！这是可行的，当然前提是，有一个人在整个过程中维护相同功能的两份配置。实际上并不难。抽一个周末空余时间，自己把这两个都熟悉了一下，整理了一套Gradle&amp;amp;Maven日常开发中常用的包和插件的集合，作为项目的开始。比较通用，所以需要根据公司或个人项目实际情况*加入私服*的配置，以及你想使用的*jar包*，如此简单。如果使用过程中遇到什么问题，请联系我。别忘了，帮我star一下。
接下来涉及到的内容：
 maven 正确使用方法 gradle正确使用方法 gradle项目和maven项目相互转化 一个项目同时支持maven和gradle配置：一个好的开始  maven 正确使用方法 maven版本不相同问题 我们大部分时候使用IDE进行项目开发的时候，大部分时候会直接使用IDE创建MAVEN项目，这是正确的。可是，您有没有发现，大家合作的时候，由于maven版本不相同，哪怕是3.5.1和3.5.2的区别，都会引发一场血案！我的可以正常打开项目，而其他人却会出现问题。除了IDE下载包损坏外，就是maven的版本不相同。其实通过一些工具，已经可以让这种情况不在发生，那就是Wrapper。请看如下图(图没配错，maven的wrapper和gradle的wrapper流程上完全相同)
前提条件：
 项目创建者系统中已经由maven的命令 其他人没有要求，mvn可有可无（原因之后说）  具体如何做：
 项目创建者执行 mvn -N io.takari:maven:wrapper -Dmaven=3.5.3  此时，项目目录会生成mvnw.cmd和mvnw，之后的所有操作都是基于此，也就是说，项目开发者不需要由任何依赖，除了jdk-_!!!  项目创建者执行 mvnw archetype:generate  此步是自动生成项目目录结构。同时，项目管理者需要搭建好基础的代码框架。之后可以开发了  项目开发者  mvnw.cmd compiler:compile mvnw.cmd exec:java -Dexec.mainClass=&amp;quot;org.exfly.LombokL.LombokLApplication&amp;quot; -q mvn.cmd clean mvn.cmd test 。。。   注意:
 当第一次执行mvnw.cmd时候，会自动下载对应版本的Maven，maven的$HOME/.m2/wrapper/dists/&amp;lt;version&amp;gt;/下。 初网络问题，如果出现错误，依赖包已经下好，只需要到1所说的位置去掉后缀.pack，重新运行即可。  使用dependencyManagement集中管理版本依赖  dependencyManagement这里已经很好的解释如何做。同时可以借鉴springboot-parent  多模块项目管理方法  多模块项目的POM重构 通过parent的方式，将多模块依赖集中管理，  如何更好的使用maven进行项目管理 几点建议  尽量使用wapper多 使用dependencyManagement集中管理版本依赖 bin下有mvn和mvnDebug(运行mvn时开始debug) M2_HOME maven主程序的安装目录 ~/.m2 本地包下载位置 http代理  setting.xml中的proxies  MAVEN_OPTS  运行mvn时候相当于运行java命令，MAVEN_OPTS可以配置为任何java的命令参数  设置MAVEN_OPTS环境变量 配置用户范围settings.xml  %M2_HOME%/conf/settings.xml 为全局配置文件 ~/.m2/settings.xml 为用户配置文件  不要使用IDE内嵌的Maven，应该配置IDE中为自己安装的maven 显示声明所有用到的依赖  我的maven常用命令笔记 我的maven常用命令笔记
gradle正确使用方法 理由同上节，直接说使用方法。可以对照我的笔记查看。
 gradle init &amp;ndash;type java-library  这里自动生成gradlew，并创建项目目录结构  之后所有命令使用gradlew即可  gradle项目和maven项目相互转化 gradle和maven可以相互转化，意味着，我们可以使用gradle为主的开发，之后导出为maven项目，供生产环境使用。前提，你足够了解gradle和maven。
maven -&amp;gt; gradle  cd /path/to/mavenproject gradle init gradle wrapper  gradle -&amp;gt; maven  cd /path/to/gradleproject gradlew install  将项目转换为maven和gradle项目后，目录结构如下： 之后，我们习惯使用mavnw或者gradlew，都可以。如此，做到了共存。
一个项目同时支持maven和gradle配置：一个好的开始 抽时间，做了常用jar包和插件整合包，一个项目同时支持maven和gradle。
共同的依赖：
 内容包括： 日志、通用工具库、单元测试、代码质量度量、文档生成等 jar: slf4j、logback、lombok、guava、junit、mockito  配置中整合的工具：
 代码质量分析报告工具：pmd、findbugs、checkstyle、jdepend 单元测试报告工具、javadoc、依赖管理、项目信息汇总等可视化信息  maven具体内容
 maven-compiler-plugin、maven-javadoc-plugin、cobertura-maven-plugin、maven-checkstyle-plugin、findbugs-maven-plugin、maven-pmd-plugin、jdepend-maven-plugin、maven-jar-plugin、maven-surefire-plugin、maven-surefire-report-plugin  gradle具体内容
 java、maven、checkstyle、pmd、findbugs、jdepend、eclipse、idea、javadoc  首先maven配置见此文件
其次gradle配置见此文件
资料汇总  完整的整合项目，支持maven和gradle，点我下载 我的Gradle笔记，点我查看 我的maven笔记，点我查看 </content>
    </entry>
    
     <entry>
        <title>OpenResty最佳实践</title>
        <url>/post/lua/openresty_awesome/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>Lua</tag><tag>OpenResty</tag>
        </tags>
        <content type="html"> 如果你正在学Lua与openresty，那你就一定知道在开发过程中，调试代码、单元测试是多么的麻烦。这里整理了一些lua开发的最佳实践。
简介 openresty中lua ide调试，单元测试比较麻烦；lua对库的管理比较散漫。在公司生产环境，一般没有外网环境，OpenResty的安装和lua项目的部署都比较麻烦。 结合Python的一些经验，在这里整理一下自己对Lua的理解，以及Lua最佳实践。
OpenResty安装 对于软件，使用编译方式安装比较好，比如Ubuntu，apt-get安装的包一般都会比较旧。如下介绍我的编译参数。这里需要自己下载自己的依赖包：naxsi, nginx-goodies-nginx-sticky-module-ng，pcre，openssl，zlib，并根据我的配置进行修改相应参数
./configure --prefix=$HOME/openresty \  --add-module=$HOME/openresty/setupfile/third/naxsi-0.55.3/naxsi_src \  --add-module=$HOME/openresty/setupfile/third/nginx-goodies-nginx-sticky-module-ng \  --with-pcre=$HOME/openresty/setupfile/depency/pcre-8.41 \  --with-openssl=$HOME/openresty/setupfile/depency/openssl-1.0.2k \  --with-zlib=$HOME/openresty/setupfile/depency/zlib-1.2.11 \  --with-http_v2_module \  --with-http_sub_module \  --with-http_stub_status_module \  --with-http_realip_module \  --with-cc-opt=-O2 \  --with-luajit  这里是一个比较好的 nginx的笔记，可以过一遍 我在学习的时候，看了这本书深入理解Nginx模块开发与架构解析,毕竟讲的比较系统，可以借鉴一下 有问题，知乎，搜索引擎  安装luarocks  下载地址 http://luarocks.github.io/luarocks/releases/ 编译安装  ./configure --prefix=$HOME/openresty/luajit \  --with-lua=$HOME/openresty/luajit \  --lua-suffix=jit \  --with-lua-include=$HOME/openresty/luajit/include/luajit-2.1 --prefix 设定 luarocks 的安装目录 --with-lua 则是系统中安装的 lua 的根目录 --lua-suffix 版本后缀，此处因为openresyt的lua解释器使用的是 luajit ,所以此处得写 jit --with-lua-include 设置 lua 引入一些头文件头文件的目录 make build &amp;amp;&amp;amp; make install lua面向对象 lua 借助table以及metatable的概念进行oo的。这里摘了一个博客的代码，看起来还可以。以后可以使用这个。Lua 中实现面向对象。 这里要说一下lua中.运算和:的区别，a={};a.fun(a, arg) 等价于 a:fun(arg)，其实就是:可以省略self参数。
local _class={} function class(super) local class_type={} class_type.ctor=false class_type.super=super class_type.new=function(...) local obj={} do local create create = function(c,...) if c.super then create(c.super,...) end if c.ctor then c.ctor(obj,...) end end create(class_type,...) end setmetatable(obj,{ __index=_class[class_type] }) return obj end local vtbl={} _class[class_type]=vtbl setmetatable(class_type,{__newindex= function(t,k,v) vtbl[k]=v end }) if super then setmetatable(vtbl,{__index= function(t,k) local ret=_class[super][k] vtbl[k]=ret return ret end }) end return class_type end 基本编码规范 设计 可以参考OpenResty的最佳实践，平时用起来，大部分跟c的风格差不多吧。主要是所使用的代码风格要统一。
包管理 lua下有两个包管理系统，LuaDist和LuaRocks
单元测试  重点 如下方法请在命令行中使用类似curl localhost/unittest进行测试，浏览器中看会很痛苦 OpenResty最佳实践-单元测试给出一种方法。我的处理方法是，在nginx.conf中的server中建一个单独的location，content_by_lua_file 设置unittest.lua。公司用的verynginx，所以我把此配置放到了router.lua中(当然配置方法类似，这个很容易研究，就不放到这里了)。  -- file: unittest.lua local _M = {} local csrf_test = require(&amp;#34;test.test_csrf&amp;#34;) local tmp_test = require(&amp;#34;test.tmp_test&amp;#34;) function _M:run_unittest() csrf_test:run() end return _M -- file: test_csrf.lua local iresty_test = require(&amp;#34;resty.iresty_test&amp;#34;) local json = require(&amp;#34;json&amp;#34;) local config = require(&amp;#34;config&amp;#34;) local csrf_config = require(&amp;#34;csrf_config&amp;#34;) local token = require(&amp;#34;token&amp;#34;) local tabletls = require(&amp;#34;tabletls&amp;#34;) local tb = iresty_test.new({unit_name=&amp;#34;test_csrf&amp;#34;}) local function assert_eq(wanted, real, msg) if wanted ~= real then error(msg or &amp;#34;error&amp;#34;, 2) -- 请注意参数 2 end end local function assert_not_eq(wanted, real, msg) if wanted == real then error(msg or &amp;#34;error&amp;#34;, 2) end end function tb:test_geturl() assert_eq(&amp;#34;/unittest&amp;#34;, ngx.var.uri, &amp;#34;the unittest url changed&amp;#34;) end function tb:run_unittest() tb:run() end return tb  如上有一个很有意思的地方，error(msg or &amp;quot;error&amp;quot;, 2),其中的2有些讲究，表示返回调用函数所在行，还有0（忽略行号），1（error调用位置行号） 性能测试 代码覆盖率 API测试等，都可以去OpenResty最佳实践中找，配置很简单。  远程调试 OpenResty  对于此部分，对于有些人来说，使用日志就已经足够了。可对于有些时候，在代码中太多的日志有不利于维护。这里自己要尽力做好日志和调试的平衡吧。 此调试方法适用于 win linux osx 先贴这里用到的luaIDE地址：ZeroBraneStudio 如下为安装步骤： 下载这个项目，ZeroBraneStudio，解压可以直接用【调试方法在下载好的文件中README.md中有相应的链接】 启动ZBS，Project -&amp;gt; Start Debugger Server 复制/lualibs/mobdebug/mobdebug.lua -&amp;gt; nginx lua path, 复制/lualibs/socket.lua -&amp;gt; nginx lua path， 复制/bin/clibs/socket/core -&amp;gt; socket设为nginx lua cpath（调试时候，使用的是require(&amp;ldquo;socket.core&amp;rdquo;)形式导入包。这里需要注意core文件后缀，win是dll，linux是so，） nginx配置好,将如上依赖加到nginx.conf中，让lua可以找到这些文件即可 创建需要调试的lua文件
require(&amp;#39;mobdebug&amp;#39;).start(&amp;#39;192.168.1.22&amp;#39;) local name = ngx.var.arg_name or &amp;#34;Anonymous&amp;#34; ngx.say(&amp;#34;Hello, &amp;#34;, name, &amp;#34;!&amp;#34;) ngx.say(&amp;#34;Done debugging.&amp;#34;) require(&amp;#39;mobdebug&amp;#39;).done()  注：start()呼叫需要运行IDE的计算机的IP 。默认情况下使用“localhost”，但是由于您的nginx实例正在运行，因此您需要指定运行IDE的计算机的IP地址（在我的例子中192.168.1.22）
 在ide中打开需要调试的如上lua文件 Project -&amp;gt; Project Directory -&amp;gt; Set From Current File。 此时，打开浏览器，访问需要此文件处理的链接 此时开始调试  注：在最下侧有Remote console，在这里可以执行任何ngx lua语句 如上流程没有截图，或者没有说清楚，可以来这里  nginx一些技巧  看我配置的nginx.conf
lua_package_path &amp;#39;$prefix/lua_script/?.lua;;&amp;#39;; 我的笔记
  资料  章亦春 OpenResty OpenResty最佳实践 Lua 5.1 参考手册 Lua 5.3 参考手册 云风 github awesome-lua awesome-resty Nginx-Lua-OpenResty-ResourcesA collection of resources covering Nginx, Nginx &#43; Lua, OpenResty and Tengine </content>
    </entry>
    
     <entry>
        <title>Spring Beans的装配规则总结</title>
        <url>/post/java/spring/beans/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>Spring</tag><tag>SSM</tag>
        </tags>
        <content type="html"> 文章简介：spring中bean的装配有一定规则，在这里进行总结。本文主要讲解一些概念和java配置方法。demo代码见文末。
目录  手动装配  使用@Bean  自动装配 使用@ComponentScan 自动装配的歧义性 条件生效 Bean profile bean作用域  手动装配 手动装配可以通过声明xml文件和java配置文件两种手段。在这两种方式中，更加推荐使用java配置的方式。两种配置不是相互替代的关系，一般将业务相关的配置放到java配置中，对于非业务，如数据库等，可以放到xml中。 通过使用@Bean注解方法，可以声明并注册一个以方法名为name的bean。@Bean(name= {&amp;ldquo;sayAndPlayServiceNewName&amp;rdquo;, &amp;ldquo;sayAndPlayService&amp;rdquo;})
public interface SayAndPlayService { String say(); String play(); } public class PeopleSayAndPlayServiceImpl implements SayAndPlayService { @Override public String say() { return &amp;#34;People Service implements say.&amp;#34;; } @Override public String play() { return &amp;#34;People Service implements play.&amp;#34;; } } @Configuration public class SimpleManualwireConfig { @Bean public SayAndPlayService sayAndPlayService() { return new PeopleSayAndPlayServiceImpl(); } } @RunWith(SpringRunner.class) @ContextConfiguration(classes= {org.exfly.demo.config.SimpleManualwireConfig.class}) public class SimpleBeanManualwireTest { @Autowired private SayAndPlayService service; @Test public void testTestOneAutowiredService() { Assert.assertEquals(&amp;#34;People Service implements say.&amp;#34;, service.say()); } @Test public void testAnnotationConfigAppContext() { ApplicationContext context = new AnnotationConfigApplicationContext(org.exfly.demo.config.SimpleManualwireConfig.class); SayAndPlayService service = (SayAndPlayService) context.getBean(&amp;#34;sayAndPlayService&amp;#34;); Assert.assertEquals(&amp;#34;People Service implements say.&amp;#34;, service.say()); } } 自动装配 使用@ComponentScan可以自动扫描，@ComponentScan告诉Spring 哪个packages 的用注解标识的类 会被spring自动扫描并且装入bean容器。自动扫描，会扫描相应包以及子包，并为所有bean生成name，name命名规则为其类首字母变小写，如interface UserService被唯一的UserServiceImpl实现，则经过扫描，bean被声明为name为userServiceImpl。如果接口被多各类实现，需要转到下文消除歧义部分进行了解。
public interface SpeakService { String speak(); } @Service public class PeopleSpeakServiceImpl implements SpeakService { @Override public String speak() { return &amp;#34;People speak&amp;#34;; } } @Configuration @ComponentScan(basePackageClasses={org.exfly.demo.service.SpeakService.class}) public class SimpleConfigScanConfig {} @RunWith(SpringRunner.class) @ContextConfiguration(classes= {org.exfly.demo.config.SimpleManualwireConfig.class}) public class SimpleBeanManualwireTest { @Test public void testAnnotationConfigAppContextAutoScan() { ApplicationContext context = new AnnotationConfigApplicationContext(org.exfly.demo.config.SimpleConfigScanConfig.class); SpeakService service = (SpeakService) context.getBean(&amp;#34;peopleSpeakServiceImpl&amp;#34;); Assert.assertEquals(&amp;#34;People speak&amp;#34;, service.speak()); } } //如果希望使用@Autowired自动装配， @RunWith(SpringRunner.class) @ContextConfiguration(classes= {org.exfly.demo.config.SimpleConfigScanConfig.class}) public class SimpleAutoScanTest { @Autowired //根据类型进行自动注入 	private SpeakService sservice; @Test public void testAnnotationConfigAppContextAutoScanAutoWire() { Assert.assertEquals(&amp;#34;People speak&amp;#34;, sservice.speak()); } } @Autowired 可以在属性、构造方法、set函数中进行自动注入
消除歧义 通过使用@Bean注解方法，可以声明并注册一个以方法名为name的bean。如果使用@Bean(name= {&amp;ldquo;sayAndPlayServiceNewName&amp;rdquo;, &amp;ldquo;sayAndPlayService&amp;rdquo;})对bean进行命名，可以用不同的名字取用（在@Autowired处再加一个@Qualifier(&amp;ldquo;sayAndPlayServiceNewName&amp;rdquo;)）; 如果使用@ComponentScan，相应的Bean定义需要使用Component等进行注解，同时使用@Qualifier(&amp;ldquo;BeanId&amp;rdquo;)限定符，如下
@Configuration public class SimpleManualwireConfig { @Bean(name={&amp;#34;sayAndPlayServiceNewName&amp;#34;, &amp;#34;sayAndPlayService&amp;#34;}) public SayAndPlayService sayAndPlayService() { return new PeopleSayAndPlayServiceImpl(); }	} @Service @Qualifier(&amp;#34;peopleSayServ&amp;#34;) public class PeopleSayServiceImpl implements SayService {} //or @Service(&amp;#34;peopleSayServ&amp;#34;) public class PeopleSayServiceImpl implements SayService {} //如何使用 @Autowired @Qualifier(&amp;#34;peopleSayServ&amp;#34;) SayService service; 条件生效 如下的解释：在当前上下文中，如果Conditional注解中的MagitExistsCondition.matches方法返回true，则当前bean：magicBean生效。@Profile和springboot自动配置都是基于此种原理实现的。
@Bean @Conditional(MagitExistsCondition.class) public MagicBean magicBean(){ return new MagitBean(); } public class MagitExistsCondition implements Condition { boolean matches(ConditionContext ctxt, AnnotatedTypeMetadat metadate){ Environment env = ctxt.getEnvironment(); return env.containsProperty(&amp;#34;magic&amp;#34;); } } @Profile bean作用域  Singleton 默认 只创建一个实例 Prototype 每次创建新的实例 Session 每个会话创建一个实例 Request 每个请求创建一个实例  使用@Scope进行配置即可
@Component @Scope(ConfigurableBeanFactory.SCOPE_PROTOTYPE) public class Notepad{} 运行时值注入 以后补充
其他 项目代码
</content>
    </entry>
    
     <entry>
        <title>Collections知识整理</title>
        <url>/post/java/collectionslearn/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>Java</tag>
        </tags>
        <content type="html"> 文章简介：学Java Collections集合，对其中一些知识进行整理
Collections结构 使用例子 Iterator public void testIterator(){ //创建一个集合  Collection books = new HashSet(); books.add(&amp;#34;轻量级J2EE企业应用实战&amp;#34;); books.add(&amp;#34;Struts2权威指南&amp;#34;); books.add(&amp;#34;基于J2EE的Ajax宝典&amp;#34;); //获取books集合对应的迭代器  Iterator&amp;lt;String&amp;gt; it = books.iterator(); while(it.hasNext()) { String book = it.next(); System.out.println(book); if (book.equals(&amp;#34;Struts2权威指南&amp;#34;)) { it.remove(); //使用Iterator迭代过程中，不可修改集合元素,下面代码引发异常  //books.remove(book);  } //对book变量赋值，不会改变集合元素本身  book = &amp;#34;测试字符串&amp;#34;; } System.out.println(books); } List 实现List接口的常用类有LinkedList，ArrayList
List&amp;lt;String&amp;gt; list = new LinkedList&amp;lt;&amp;gt;(); Set Set接口有以下几种实现：
 HashSet : 为快速查找设计的Set，主要的特点是：不能存放重复元素，而且采用散列的存储方法，所以没有顺序。这里所说的没有顺序是指元素插入的顺序与输出的顺序不一致。 TreeSet : 保存次序的Set, 底层为树结构。使用它可以从Set中提取有序的序列。 LinkedHashSet : 具有HashSet的查询速度，且内部使用链表维护元素的顺序(插入的次序)。于是在使用迭代器遍历Set时，结果会按元素插入的次序显示。
Set&amp;lt;String&amp;gt; hs = new HashSet&amp;lt;&amp;gt;();  Map Map接口有以下几种实现： HashMap、LinkedHashMap、HashTable和TreeMap
Map&amp;lt;String, String&amp;gt; m1 = new HashMap&amp;lt;&amp;gt;(); m1.put(&amp;#34;Zara&amp;#34;, &amp;#34;8&amp;#34;); m1.get(&amp;#34;Zara&amp;#34;); // 8 m1.containsKey(&amp;#34;Zara&amp;#34;); // true  Java8的HashMap详解（存储结构，功能实现，扩容优化，线程安全，遍历方法）  Queue Queue&amp;lt;String&amp;gt; queue = new LinkedList&amp;lt;String&amp;gt;(); //添加元素 queue.offer(&amp;#34;a&amp;#34;); queue.offer(&amp;#34;b&amp;#34;); queue.offer(&amp;#34;c&amp;#34;); queue.offer(&amp;#34;d&amp;#34;); queue.offer(&amp;#34;e&amp;#34;); for(String q : queue){ System.out.println(q); } System.out.println(&amp;#34;===&amp;#34;); System.out.println(&amp;#34;poll=&amp;#34;&#43;queue.poll()); //返回第一个元素，并在队列中删除 for(String q : queue){ System.out.println(q); } System.out.println(&amp;#34;===&amp;#34;); System.out.println(&amp;#34;element=&amp;#34;&#43;queue.element()); //返回第一个元素 for(String q : queue){ System.out.println(q); } System.out.println(&amp;#34;===&amp;#34;); System.out.println(&amp;#34;peek=&amp;#34;&#43;queue.peek()); //返回第一个元素 for(String q : queue){ System.out.println(q); } /* a b c d e === poll=a b c d e === element=b b c d e === peek=b b c d e */ 转成线程安全 List&amp;lt;String&amp;gt; list = Collections.synchronizedList(new LinkedList&amp;lt;&amp;gt;()); 资源  官方 Collections Api reference Java集合框架面试题 比较细致的讲解 面试整理-Java综合高级篇（吐血整理） 最全的BAT大厂面试题整理 </content>
    </entry>
    
     <entry>
        <title>Wox&#43;Everything改变日常使用电脑的流程神器，墙裂推荐</title>
        <url>/post/tools/wox_everything/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>工具</tag>
        </tags>
        <content type="html"> 文章简介：你可以将 Wox 看作一个高效的本地快速搜索框，通过快捷键呼出（默认alt&#43;空格），然后输入关键字（支持拼音模糊查询）来搜索程序进行快速启动，或者搜索本地硬盘的文件，打开百度、Google 进行搜索，甚至是通过一些插件的功能实现单词翻译、关闭屏幕、查询剪贴板历史、查询编程文档、查询天气等更多功能。
软件准备  everything wox  介绍一些软件的特性  直接使用搜索引擎搜索 可以搜索软件，直接回车即可运行 搜索文件、文件夹，回车后使用系统默认软件打开 配置系统命令，可以直接运行(类似Win&#43;R) 通过插件可以实现单词翻译等功能  安装方法 链接中有具体的使用方法。我更喜欢绿色软件，下载下来直接可以使用。
安装之后 看一下使用效果： 引用  Wox一款国产开源的快捷启动器辅助工具神器 具体怎么配置可以看这个 wox程序开源地址 </content>
    </entry>
    
     <entry>
        <title>使用gensim训练word2vec模型--中文维基百科语料</title>
        <url>/post/algorithm/wiki_zh_practice_word2vec/</url>
        <categories>
          
        </categories>
        <tags>
          
        </tags>
        <content type="html"> 文章简介：为了写论文，使用gensim训练word2vec模型，如下记录了进行训练的过程
准备  中文维基百科预料：zhwiki-latest-pages-articles.xml.bz2 python3 wiki_zh_word2vec 繁体转简体：opencc一定要下*-win32.7z,win64的在我电脑上无法运行。如果使用我的wiki_zh_word2vec,则项目中包含可以直接使用的opencc  TODO 依赖准备  下载中文维基百科预料 git clone https://github.com/ExFly/wiki_zh_word2vec.git 将zhwiki-latest-pages-articles.xml.bz2放到build文件夹下 cd path/to/wiki_zh_word2vec pip install pipenv pipenv install &amp;ndash;dev  将XML的Wiki数据转换为text格式  pipenv run python 1_process.py build/zhwiki-latest-pages-articles.xml.bz2 build/wiki.zh.txt  31分钟运行完成282855篇文章，得到一个931M的txt文件
中文繁体替换成简体  opencc-1.0.1-win32/opencc -i build/wiki.zh.txt -o build/wiki.zh.simp.txt -c opencc-1.0.1-win32/t2s.json  大约使用了15分钟
结巴分词  pipenv run python 2_jieba_participle.py  大约使用了30分钟
Word2Vec模型训练  pipenv run python 3_train_word2vec_model.py  大约使用了30分钟，且全程cpu使用率达到90%&#43;
模型测试  pipenv run python 4_model_match.py  d:\Project\wiki_zh_word2vec(develop) λ pipenv run python 4_model_match.py 国际足球 0.5256255865097046 足球队 0.5234458446502686 篮球 0.5108680725097656 足球运动 0.5033905506134033 国家足球队 0.494105726480484 足球比赛 0.4919792115688324 男子篮球 0.48382389545440674 足球联赛 0.4837716817855835 体育 0.4835757911205292 football 0.47945135831832886 查看结果 可以使用linux的head或者tail命令查看运行的结果。 * head -n 100 wiki.zh.simp.txt &amp;gt; wiki.zh.simp_head_100.txt,直接查看wiki.zh.simp_head_100.txt即可 * 没有head命令，可以安装gow，或者直接下载cmder,进入就可以使用head命令了
结果  至此，使用python对中文wiki语料的词向量建模就全部结束了，wiki.zh.text.vector中是每个词对应的词向量，可以在此基础上作文本特征的提取以及分类。所有代码都已上传至本人GitHub中，欢迎指教！ 感谢AimeeLee77,其代码为Python2，我的项目exfly/wiki_zh_word2vec已经完全迁移到python3,并向AimeeLee77提交了pull request wiki_zh_word2vec </content>
    </entry>
    
     <entry>
        <title>2017年终总结，未来规划</title>
        <url>/post/diary/2018.02.01-2017-holiday-summary/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>总结</tag>
        </tags>
        <content type="html"> 对2017年全年的总结，并为未来做一些安排。
年前一天，依旧忙碌 往事 如今 未来 以后该怎么如何进步？
其他 继续学习，共勉！
</content>
    </entry>
    
     <entry>
        <title>Vim学习</title>
        <url>/post/tools/vim/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>linux</tag><tag>工具</tag>
        </tags>
        <content type="html"> 学习vim基本使用的笔记
引言 想学一下Vim的键位，结合sublime text和vscode的vim插件加快编码速度编码
正文 资源 使用方法  Vim 简体中文  cheatsheet  vim_cheat_sheet vim_表格形式 vim_脑图形式 vim_中文形式    </content>
    </entry>
    
     <entry>
        <title>个人密码管理&#43;Android装机指南</title>
        <url>/post/sercurity/2017.12.19-%E4%B8%AA%E4%BA%BA%E5%AF%86%E7%A0%81%E7%AE%A1%E7%90%86-android%E8%A3%85%E6%9C%BA%E6%8C%87%E5%8D%97/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>安全</tag>
        </tags>
        <content type="html"> 分享自己的密码管理体系:keepass&#43;坚果云&#43;keepass2Android，以及使用容器，搭建防止手机越用越卡的日常使用app体系，该应用不需要root。
个人密码管理 准备  win:keepass 自助云存储:坚果云账号 Android:keepass2Android  开始 工具准备好就开始吧，按如下步骤：
 电脑端keepass本地建一个密码文件（需要一个主密码，以后可以修改，主密码一定要复杂），后上传到坚果云里，（这时候就可以删除本地的密码文件了）； 坚果云中配置第三方应用授权。（坚果云记得开二步验证，这样每次登陆需要微信接收验证码才可以登陆，更安全一些）； 电脑keepass打开url，以及Android手机keepass2Android打开url； 完。  具体如何创建请看这个链接， 如我这般建密码维护基本不会出现问题。
到这里密码管理体系基本完成了。可是对于一个新手机，用好久就会卡，很卡，超级卡，为了解决这个问题，就要耍一些小手段，具体如下。
Android装机指南 准备  容器开源产品，基于virtualapp框架，有点像Docker；又是双开工具，他自己有自己的运行环境；又可以说是Android下的免安装应用的运行平台。之后会告诉你怎么用，超级棒 酷安应用商店 apkpure.com下载中国下载不了的应用，有一个没被墙的网址，下载好app后，app不需要vpn，懂了没？域名没找到，懒得找了。google商店里所有的应用这里都可以下，自己想像吧  开始 先截个图，看一看 最屌炸天的是，我把淘宝、王者荣耀和吃鸡都放到容器了，而且完全没有性能损失
基本思路：把必须装到手机里的（比如支付宝，微信等）装到手机里，非必需（比如淘宝，百度云等），都装容器里。
类比真机，软件需要有一个执行环境和临时文件，容器里的软件文件都存到了/virtual中的。
使用思路：平时把可以放到容器中的软件放到里边，不用的时候直接关闭容器，容器里所有的软件会关闭。这样就防止软件的后台自动启动，浪费内存，手机越用越慢的现象。
具体看我平时常用的一些软件，我把他们分为主机（就是安装到真机中）和容器中的。地址分别如下
 真机 容器  最后 有什么问题请直接在我的酷安@我，或者邮箱我：exflycat@gmail.com。
Enjoy!!!
</content>
    </entry>
    
     <entry>
        <title>oneplus one 刷Kali Linux NetHunter</title>
        <url>/post/sercurity/2017.09.17-black-phone/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>安全</tag><tag>kali</tag><tag>刷机</tag>
        </tags>
        <content type="html"> 看了余弦大大的知乎live后，发现真的需要对自己的隐私安全做点什么了。一激动，淘宝500大洋买了一部oneplus one，刷kali。 这里记录使用oneplus one手机打造黑手全过程，以及一些使用到的资源，以及经验汇总。
一 使用oneplus one搭建黑手还是比较简单的，因为对Android平台不熟悉，刷kali过程中踩了许多坑。比如二清、三清、四清等等。如下一步一步的说如何刷。
二 刷机四步走 第一步， 仔细阅读官方Wiki 仔细研读仔细阅读官方Wiki，可以减少刷机过程中各种坑。
第二步， 刷TWRP-oneplus1 一句话总结就是：解锁、刷进对应的 twrp.img。整个过程本质是一加 3T 开启了开发者模式，同时电脑上基于预配置好的adb、fastboot命令完成这一系列操作。这个&amp;rdquo;预配置&amp;rdquo;在 Windows 下，也可以参考&amp;rdquo;Bacon Root Toolkit&amp;ldquo;（这是专为一加打造的GUI工具集，当时还是一加1时，用这个很方便，虽然很久没更新了，但作为参考还是很好的）。
第三步，下载最新的 NetHunter，并进入TWRP的recovery模式刷入kali kernel-nethunter-[device]-[os]-*.zip nethunter-generic-[arch]-kalifs-*.zip 对于oneplus1手机来说，其对应的[arch]为armhf。 随后进行刷入操作。进入TWRP，选择安装。先找到CM13.0刷到oneplus中，后进行默认的WIPE。之后再TWRP安装中选择kernel-nethunter-[device]-[os]-*.zip，安装结束后，在选择nethunter-generic-[arch]-kalifs-*.zip，最后这个文件安装话费的时间比较久，大约10多分钟的样子。
第四步，都刷顺利后，开机进入kali ，用已经“预装”上的 SuperSU App 来完成之后一系列的 Root授权即可。 三 如果安装Kali Linux NetHunter,需要下载的文件如下：
 CM13.0 TWRP：第三方recovery brt：oneplus的解锁、root工具 Kali Linux NetHunter：如果刷其他系统，可能需要的文件如下： TWRP：第三方recovery lineageos rom：cm的重生 supersu：root工具  四 Enjoy！
最后 安装了Kali Linux NetHunter，手机便有了完整的python环境。剩下的，你懂的。
</content>
    </entry>
    
     <entry>
        <title>各种排序算法实现方法</title>
        <url>/post/algorithm/sort/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>算法</tag>
        </tags>
        <content type="html"> 总结一些常用的排序算法，如冒泡排序、插入排序、快速排序、计数排序、二分排序、归并排序等。
Source  排序算法比较-wiki visualgo 排序算法的动态图 排序算法复杂度   冒泡排序 伪代码 function bubble_sort (array, length) { var i, j; for(i from 0 to length-1){ for(j from 0 to length-1-i){ if (array[j] &amp;gt; array[j&#43;1]) swap(array[j], array[j&#43;1]) } } } 函数 冒泡排序 输入 一个数组名称为array 其长度为length i 从 0 到 (length - 1) j 从 0 到 (length - 1 - i) 如果 array[j] &amp;gt; array[j &#43; 1] 交换 array[j] 和 array[j &#43; 1] 的值 如果结束 j循环结束 i循环结束 函数结束  python实现 def bubble(List): for j in range(len(List)-1,0,-1): for i in range(0, j): if List[i] &amp;gt; List[i&#43;1]: List[i], List[i&#43;1] = List[i&#43;1], List[i] return List 插入排序 python实现 def insert_sort(lst): n=len(lst) if n==1: return lst for i in range(1,n): for j in range(i,0,-1): if lst[j] &amp;lt; lst[j-1]: lst[j], lst[j-1] = lst[j-1], lst[j] return lst 快速排序 python实现 def quicksort(a): if len(a) == 1: return a[0] if len(a) &amp;lt; 1: return 0 return quicksort([x for x in a[1:] if x &amp;lt; a[0]]), [a[0]], quicksort([x for x in a[1:] if x &amp;gt; a[0]]) C语言实现 int partition(int arr[], int low, int high){ int key; key = arr[low]; while(low &amp;lt; high){ while(low &amp;lt; high &amp;amp;&amp;amp; arr[high]&amp;gt;= key ) high--; if(low &amp;lt; high) arr[low&#43;&#43;] = arr[high]; while( low &amp;lt; high &amp;amp;&amp;amp; arr[low]&amp;lt;=key ) low&#43;&#43;; if(low &amp;lt; high) arr[high--] = arr[low]; } arr[low] = key; return low; } void quick_sort(int arr[], int start, int end){ int pos; if (start &amp;lt; end){ pos = partition(arr, start, end); quick_sort(arr,start,pos-1); quick_sort(arr,pos&#43;1,end); } return; } 归并排序 python实现 from collections import deque def merge_sort(lst): if len(lst) &amp;lt;= 1: return lst def merge(left, right): merged,left,right = deque(),deque(left),deque(right) while left and right: merged.append(left.popleft() if left[0] &amp;lt;= right[0] else right.popleft()) # deque popleft is also O(1) merged.extend(right if right else left) return list(merged) middle = int(len(lst) // 2) left = merge_sort(lst[:middle]) right = merge_sort(lst[middle:]) return merge(left, right) 计数排序 C实现 void counting_sort(int *ini_arr, int *sorted_arr, int n) { int *count_arr = (int *) malloc(sizeof(int) * 100); int i, j, k; for (k = 0; k &amp;lt; 100; k&#43;&#43;) count_arr[k] = 0; for (i = 0; i &amp;lt; n; i&#43;&#43;) count_arr[ini_arr[i]]&#43;&#43;; for (k = 1; k &amp;lt; 100; k&#43;&#43;) count_arr[k] &#43;= count_arr[k - 1]; for (j = n; j &amp;gt; 0; j--) sorted_arr[--count_arr[ini_arr[j - 1]]] = ini_arr[j - 1]; free(count_arr); } 二分查找 int binary_search(int array[],int n,int value){ int left=0; int right=n-1; while (left&amp;lt;=right){ int middle=left &#43; ((right-left)&amp;gt;&amp;gt;1); //防止溢出，移位也更高效。同时，每次循环都需要更新。 if (array[middle] &amp;gt; value) { right =middle-1; } else if(array[middle] &amp;lt; value) { left=middle&#43;1; } else { return middle; } } return -1; } </content>
    </entry>
    
     <entry>
        <title>2016年终总结，未来规划</title>
        <url>/post/diary/2017.01.01-2016-holiday-summary/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>总结</tag>
        </tags>
        <content type="html"> 对2016年全年的总结，并为未来做一些安排。
年前一天，依旧忙碌 往事 14年十月至今天，从0到1的转变。 开始的时候，不断的学习各种软件的使用，像什么PS之类的东西玩了个遍，总感觉这些东西都是别人做出来的东西，便生出自己也搞出一些想这类软件。然而继续的瞎搞。 不知什么时候，知道通过学习c可以做出来好多有意思的小玩意，比如贪吃蛇，便开始学习高级语言。 之后，知道原来学习计算机，需要系统的学习计算机的理论知识，从此入坑，一发不可收拾。 网易云课堂是自己计算机启蒙课程。跟着上边的课程学了好久，懂了计算机需要如何入门。学了导论、计组、C语言、算法和数据结构、操作系统、计算机网络、数据库原理等等。自己找视频看了Linux，读了鸟哥的linux书，甚至直接吧自己的电脑系统换成了Ubuntu/linuxmint，真正体会到Linux该如何使用。 假期用Flask完成了动态网站，部署到服务器里一段时间，后来因为网站太简单，给撤了。 后来深入web，用了两个月的时间，把前端学了一下，同时完成了自己第一个网页。学了Tornado,重构了几次Project Generator，也算初步掌握全栈开发。同学科研训练，找了一个Tornado开源项目，改成了论坛系统，还帮这个项目修了几个bug。 学了些东西。
如今 如今，趁着假期，也是准备考研前的最后一个可以自由支配的假期，准备了些东西回来学。每天完成一个汇编的项目，( 假期汇编 )。准备好好学学数据结构和算法导论.
越努力越迷茫 越努力，知道的越多，不知到的更多。为了选择自己的技术方向，越接触越不知道该学什么，越着急，越急躁。 同时还有一些其他的事让我烦心。慢慢来吧，急不来。
未来 以后该怎么如何进步？
其他 继续学习，共勉！
</content>
    </entry>
    
     <entry>
        <title>Linux0.11源码学习环境配置与相关资源汇总</title>
        <url>/post/linux/linux0.11-read-code/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>Linux</tag><tag>资源</tag>
        </tags>
        <content type="html"> 记录学习Linux操作系统实现时候使用过的资源，一部分笔记，以及调试中用到的技巧。内容有点乱，仅供个人使用。
linux history  linux 0.11 linux 0.95 实现虚拟文件系统 linux 0.96 实现网络接口  linux linux采用分段&#43;分页机制结合管理内存
linux 调试方法  linux0.11 调试方法
gdb tools/system target remote localhost:1234 gdb常用命令 b: 下中斷點 info b :u 列出目前中断点，也可简写成&amp;#34;i b&amp;#34; continue(c) 继续执行直到下一个中断点或结束 list(l): 列出目前上下文 step(s): 单步 (会进入 funciton) next(n) : 单步 (不会进入 funciton) until(u) 跳离一个 while for 循环 print(p): 显示某变量，如 p str info register(i r) : 显示 CPU 的 register GDB 打印出内存中的內容，格式為 x/nyz，其中 n: 要印出的數量 y: 显示的格式，可为C( char), d(整数), x(hex) z: 单位，可为 b(byte), h(16bit), w(32bit) cgdb	可显示为上半部分为代码，下半部分命令部分 cgdb tools/system* [linux-0.11启动过程描述](http://labrick.cc/2015/08/13/linux-0-11-boot/) * [Linux0.11启动过程](http://linux.chinaunix.net/techdoc/install/2007/04/10/954810.shtml) * [80386保护模式的本质](http://www.jianshu.com/p/1cea7dc5d6b7) * [linux虚拟地址到线性地址的转化](http://luodw.cc/2016/02/17/address/) * [Linux内存寻址之分段机制-linux回避了分段机制](http://blog.xiaohansong.com/2015/10/03/Linux%E5%86%85%E5%AD%98%E5%AF%BB%E5%9D%80%E4%B9%8B%E5%88%86%E6%AE%B5%E6%9C%BA%E5%88%B6/) * [Linux内存寻址之分页机制/](http://blog.xiaohansong.com/2015/10/05/Linux%E5%86%85%E5%AD%98%E5%AF%BB%E5%9D%80%E4%B9%8B%E5%88%86%E9%A1%B5%E6%9C%BA%E5%88%B6/) * [逻辑地址、线性地址、物理地址和虚拟地址](http://www.voidcn.com/blog/will130/article/p-5705051.html) * [Intel 80386 程序员参考手册](http://www.kancloud.cn/wizardforcel/intel-80386-ref-manual/123838) * [linux0.11内核之文件系统](http://harpsword.leanote.com/post/Untitled-563d6103ab6441584f000164)  source 80386内存访问公式 32位线性地址 = 段基地址(32位) &#43; 段内偏移(32位)
48bit = 16 &#43; 32 16位地段选择子 &#43; 32虚拟地址 -&amp;gt; 32线性地址 32线性地址 -&amp;gt; 物理地址
</content>
    </entry>
    
</search>